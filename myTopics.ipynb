{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn import metrics, tree\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_selection import chi2, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.tsv', sep='\\t', quoting=csv.QUOTE_NONE, dtype=str, encoding = 'utf-8',\n",
    "                 header=None, names=[\"instance\", \"text\", \"id\", \"sentiment\", \"is_sarcastic\"])\n",
    "# df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.array([])\n",
    "# Read tweets\n",
    "for text in df.text:\n",
    "    text_data = np.append(text_data, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Functions for text pre-processing \"\"\"\n",
    "\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \" \", sample)\n",
    "\n",
    "\n",
    "def remove_punctuation(sample):\n",
    "    \"\"\"Remove punctuations from a sample string\"\"\"\n",
    "#     punctuations = r'''$!\"&'()*+,-./:;<=>?[\\]^`{|}~'''\n",
    "#     no_punct = \"\"\n",
    "#     for char in sample:\n",
    "#         if char not in punctuations:\n",
    "#             no_punct = no_punct + char\n",
    "#     return no_punct\n",
    "    return re.sub(r'[^\\w\\s\\&\\#\\@\\$\\%\\_]','',sample)\n",
    "\n",
    "def myTokenizer(sample):\n",
    "    \"\"\"Customized tokenizer\"\"\"\n",
    "    ################################## 1. Remove numbers\n",
    "    ################################## 2. Remove auspoll thingy\n",
    "    ################################## 3. Remove starts with au\n",
    "    new_words = []\n",
    "    words = sample.split(' ')\n",
    "    new_words = [word for word in words if len(word) >= 2 and not word.isdigit() and not word.startswith('#aus') and not word.startswith('au')]\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords_NLTK(sample):\n",
    "    \"\"\"Remove stopwords using NLTK\"\"\"\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    filteredText = \"\"\n",
    "    for word in words:\n",
    "        if word not in stopWords:\n",
    "            filteredText = filteredText + word + \" \"\n",
    "    return filteredText.rstrip()\n",
    "\n",
    "\n",
    "def porter_stem(sample):\n",
    "    \"\"\"Stemming\"\"\"\n",
    "    words = myTokenizer(sample)\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + ps.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "\n",
    "def myPreprocessor(sample):\n",
    "    \"\"\"Customized preprocessor\"\"\"\n",
    "    sample = remove_URL(sample)\n",
    "    sample = sample.lower()\n",
    "    sample = remove_stopwords_NLTK(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "    sample = porter_stem(sample)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myTokenizer('While the majority of all Internet search engines utilize stop words, they do not prevent a user from using them, but they are ignored.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#4corner', '#7new', '#abc730', '#abcnews24', '#afpraid', '#agchatoz', '#alp', '#asylumseek', '#betterfutur', '#blackhol', '#bootturnbullout', '#brexit', '#budget2016', '#cfa', '#cfmeu', '#chafta', '#childcar', '#cleanenergi', '#climat', '#climatechang', '#coal', '#corrupt', '#csg', '#csiro', '#csirocut', '#dutton', '#educ', '#election2016', '#environ', '#et', '#faketradi', '#fraudband', '#gonski', '#greatbarrierreef', '#green', '#greens16', '#humanright', '#icac', '#inequ', '#insid', '#labor', '#laborlaunch', '#latelin', '#leadersdeb', '#liber', '#lnp', '#lnpfail', '#malcolm', '#malwar', '#marriageequ', '#medicar', '#nauru', '#nbn', '#nbnco', '#nbngate', '#ndi', '#negativegear', '#npc', '#nswpol', '#panamapap', '#parakeelia', '#peoplesforum', '#pmlive', '#polita', '#putlnplast', '#qanda', '#qldpol', '#refuge', '#renew', '#safeschool', '#savemedicar', '#scomo', '#spendomet', '#springst', '#ssm', '#stopstateterror', '#taxrort', '#tennew', '#thedrum', '#turnbul', '$$$', '$2', '$50b', '$50bn', '$billion', '&amp', '&gt', '100%', '1st', '2nd', '50%', '@2gbnew', '@abcnew', '@abcnews24', '@albomp', '@australianlabor', '@barnaby_joyc', '@billshortenmp', '@bowenchri', '@cpyne', '@green', '@greghuntmp', '@johndory49', '@juliebishopmp', '@liberalau', '@margokingston1', '@mathiascormann', '@peterdutton_mp', '@richarddinatal', '@scottmorrisonmp', '@skynewsaust', '@smh', '@tanya_plibersek', '@theag', '@tonyabbottmhr', '@tonyhwindsor', '@turnbullmalcolm', '@unionsaustralia', 'aaa', 'abbott', 'abc', 'abl', 'abort', 'absolut', 'abus', 'account', 'act', 'action', 'actual', 'ad', 'adani', 'address', 'admit', 'adopt', 'afford', 'afp', 'again', 'age', 'agenda', 'agent', 'agil', 'ago', 'agre', 'aid', 'all', 'alleg', 'allianc', 'allow', 'alon', 'alp', 'alreadi', 'also', 'alway', 'analysi', 'andrew', 'anim', 'announc', 'anoth', 'answer', 'anyon', 'anyth', 'approv', 'around', 'ask', 'assist', 'asylum', 'attack', 'attend', 'avoid', 'away', 'back', 'backward', 'bad', 'ban', 'bank', 'banker', 'bark', 'barnabi', 'barrier', 'base', 'basic', 'bc', 'becom', 'behind', 'believ', 'benefit', 'best', 'better', 'beyond', 'big', 'bigger', 'biggest', 'bill', 'billion', 'bird', 'bishop', 'bit', 'black', 'blah', 'blame', 'blind', 'block', 'blood', 'bloodi', 'blow', 'boat', 'boost', 'border', 'boss', 'bottom', 'bowen', 'break', 'brexit', 'bring', 'britain', 'broadband', 'broke', 'broken', 'budget', 'build', 'bulk', 'bulkbil', 'bullshit', 'busi', 'buy', 'call', 'camp', 'campaign', 'candid', 'cannot', 'cant', 'carbon', 'care', 'cash', 'cattl', 'caus', 'cayman', 'centr', 'ceo', 'cfa', 'chang', 'check', 'child', 'childcar', 'children', 'choic', 'chri', 'claim', 'class', 'clean', 'clear', 'clearli', 'climat', 'club', 'co', 'coal', 'coalit', 'come', 'comment', 'commiss', 'commit', 'commun', 'compani', 'competit', 'concern', 'confid', 'confirm', 'consid', 'contact', 'continu', 'copper', 'cormann', 'corp', 'corpor', 'corrupt', 'cost', 'could', 'council', 'countri', 'coz', 'creat', 'credibl', 'credit', 'crimin', 'crisi', 'criticis', 'csiro', 'cut', 'dairi', 'damag', 'day', 'deal', 'death', 'debat', 'debt', 'decid', 'decis', 'declar', 'defend', 'deficit', 'deliv', 'democraci', 'demonstr', 'deni', 'dental', 'dept', 'deserv', 'desper', 'destroy', 'detail', 'detent', 'develop', 'di', 'die', 'differ', 'digit', 'dinner', 'direct', 'directli', 'disgrac', 'disput', 'distract', 'doctor', 'document', 'dodgi', 'dollar', 'donat', 'done', 'donor', 'dont', 'drive', 'dud', 'due', 'dutton', 'earli', 'earn', 'earner', 'econom', 'economi', 'ed', 'educ', 'effect', 'either', 'elect', 'elector', 'electr', 'end', 'energi', 'english', 'ensur', 'equal', 'etc', 'eu', 'even', 'ever', 'everi', 'everyon', 'everyth', 'evid', 'exactli', 'examin', 'expect', 'expens', 'experi', 'expert', 'explain', 'export', 'expos', 'extra', 'extremist', 'eye', 'face', 'fact', 'fail', 'failur', 'fair', 'fake', 'fall', 'famili', 'far', 'farmer', 'fast', 'fault', 'fear', 'fed', 'feder', 'fee', 'feel', 'ff', 'fiasco', 'fibr', 'fight', 'figur', 'final', 'financ', 'find', 'firefight', 'first', 'fiscal', 'fix', 'flow', 'folk', 'follow', 'fonseca', 'fool', 'for', 'forc', 'foreign', 'forest', 'forget', 'former', 'found', 'free', 'freez', 'front', 'fta', 'full', 'fulli', 'fund', 'funnel', 'furiou', 'futur', 'ga', 'gain', 'gay', 'gdp', 'gear', 'gener', 'get', 'gfc', 'gift', 'gillard', 'give', 'given', 'global', 'go', 'goe', 'gone', 'good', 'got', 'gov', 'govern', 'govt', 'gp', 'great', 'green', 'greg', 'grow', 'growth', 'gst', 'guarante', 'half', 'halt', 'hand', 'happen', 'hard', 'hate', 'haven', 'he', 'head', 'headlin', 'headspac', 'health', 'healthcar', 'hear', 'heard', 'heart', 'held', 'help', 'here', 'hey', 'hide', 'high', 'higher', 'histori', 'hit', 'hockey', 'hole', 'home', 'homeless', 'hope', 'hopeless', 'hospit', 'hous', 'housingfair', 'human', 'hung', 'hunt', 'hurt', 'id', 'idea', 'ideolog', 'ignor', 'ill', 'illeg', 'illiter', 'im', 'imagin', 'immigr', 'impact', 'import', 'in', 'increas', 'independ', 'indonesia', 'industri', 'inform', 'infrastructur', 'innov', 'innumer', 'insignific', 'instead', 'intak', 'integr', 'interest', 'intern', 'internet', 'internship', 'introduc', 'invest', 'investig', 'investor', 'involv', 'iron', 'is', 'islam', 'islamist', 'island', 'isnt', 'issu', 'it', 'job', 'john', 'joke', 'joyc', 'juli', 'keep', 'kelli', 'kid', 'kill', 'knew', 'know', 'known', 'labor', 'lack', 'last', 'launder', 'law', 'lead', 'leader', 'leak', 'learn', 'left', 'legal', 'less', 'let', 'level', 'liar', 'lib', 'liber', 'lie', 'like', 'limit', 'line', 'link', 'listen', 'littl', 'live', 'lnp', 'loan', 'local', 'lock', 'lol', 'long', 'look', 'lose', 'lost', 'lot', 'love', 'low', 'lower', 'mad', 'made', 'mafia', 'major', 'make', 'mal', 'malcolm', 'man', 'manag', 'mani', 'manu', 'manufactur', 'margin', 'mark', 'market', 'marriag', 'marshal', 'mate', 'mathia', 'may', 'me', 'mean', 'measur', 'media', 'medic', 'medicar', 'member', 'mental', 'mention', 'migrant', 'million', 'mine', 'minimum', 'minist', 'minor', 'mismanag', 'mobil', 'model', 'money', 'more', 'morn', 'morrison', 'mossack', 'move', 'mp', 'mr', 'msm', 'mt', 'much', 'multin', 'murdoch', 'muslim', 'must', 'name', 'natal', 'nation', 'nauru', 'nbn', 'need', 'neg', 'never', 'new', 'news', 'next', 'no', 'not', 'noth', 'now', 'nsw', 'obsess', 'off', 'offer', 'offic', 'offici', 'offshor', 'ok', 'old', 'on', 'one', 'open', 'opportun', 'oppos', 'opposit', 'order', 'otherwis', 'out', 'outcom', 'outsourc', 'oversea', 'owner', 'oz', 'pa', 'packag', 'paid', 'panama', 'paper', 'parakeelia', 'parent', 'parliament', 'part', 'parti', 'pass', 'past', 'patient', 'pay', 'payment', 'penalti', 'pension', 'peopl', 'per', 'person', 'perth', 'peter', 'photo', 'pl', 'plan', 'pleas', 'plebiscit', 'pledg', 'pm', 'pocket', 'point', 'polic', 'polici', 'polit', 'politician', 'poll', 'poor', 'posit', 'power', 'ppl', 'pretend', 'prevent', 'price', 'prime', 'privat', 'privatis', 'problem', 'profit', 'program', 'project', 'promis', 'promot', 'proof', 'properti', 'prosper', 'protect', 'protest', 'prove', 'provid', 'public', 'pull', 'put', 'pyne', 'question', 'quit', 'racism', 'raid', 'rais', 'rate', 'rba', 're', 'read', 'readi', 'real', 'realiti', 'realli', 'reason', 'rebat', 'record', 'reduc', 'reef', 'reform', 'refuge', 'refus', 'rein', 'releas', 'rememb', 'remov', 'renew', 'report', 'requir', 'research', 'respect', 'respond', 'respons', 'restor', 'return', 'reveal', 'revers', 'rhetor', 'rich', 'right', 'rise', 'risk', 'road', 'roll', 'rort', 'royal', 'rt', 'rule', 'run', 'sa', 'sack', 'safe', 'said', 'sale', 'samesex', 'save', 'say', 'sb', 'scandal', 'scare', 'school', 'scott', 'scrap', 'screw', 'sea', 'seat', 'second', 'secret', 'sector', 'secur', 'see', 'seek', 'seeker', 'seem', 'sell', 'senat', 'seriou', 'servic', 'set', 'shame', 'share', 'shift', 'shit', 'shock', 'shonki', 'shoot', 'shorten', 'show', 'shred', 'shut', 'side', 'sign', 'silent', 'sinc', 'singl', 'skill', 'sky', 'slash', 'slogan', 'slow', 'sm', 'small', 'so', 'social', 'societi', 'solut', 'someth', 'soon', 'sort', 'sound', 'south', 'speak', 'spend', 'spent', 'spoke', 'sport', 'ssm', 'stabil', 'staff', 'stand', 'start', 'state', 'statement', 'still', 'stop', 'stori', 'strategi', 'strong', 'student', 'stuff', 'stupid', 'submarin', 'subsidi', 'suicid', 'super', 'superannu', 'support', 'sure', 'surplu', 'surpris', 'system', 'tafe', 'take', 'talk', 'target', 'tax', 'taxat', 'taxpay', 'team', 'tell', 'ten', 'term', 'terror', 'terrorist', 'test', 'thank', 'that', 'the', 'them', 'there', 'theyll', 'theyr', 'thi', 'thing', 'think', 'though', 'thought', 'thousand', 'threat', 'till', 'time', 'today', 'told', 'toni', 'took', 'top', 'total', 'trade', 'transfer', 'transit', 'transpar', 'transport', 'treasur', 'treasuri', 'treat', 'tri', 'trickl', 'true', 'truli', 'trump', 'trust', 'trustpm', 'truth', 'turn', 'turnbul', 'two', 'uk', 'unemploy', 'uni', 'union', 'urg', 'us', 'use', 'usual', 'valu', 'via', 'victoria', 'view', 'visa', 'visit', 'volunt', 'vote', 'voter', 'wa', 'wage', 'wait', 'wake', 'want', 'war', 'warn', 'wast', 'watch', 'water', 'way', 'we', 'wealthi', 'week', 'welfar', 'well', 'were', 'weve', 'what', 'whether', 'white', 'who', 'whole', 'win', 'within', 'without', 'women', 'wonder', 'wont', 'word', 'work', 'worker', 'world', 'wors', 'worst', 'worth', 'would', 'wow', 'wrap', 'write', 'wrong', 'xenophobia', 'ya', 'ye', 'yeah', 'year', 'yesterday', 'yet', 'you', 'young', 'your', 'yr', 'zero']\n",
      "1000\n"
     ]
    }
   ],
   "source": [
    "# try to use sklearn stop_words later\n",
    "count = CountVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer, max_features=1000)\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "print(count.get_feature_names())\n",
    "size = len(count.vocabulary_)\n",
    "print(len(count.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_words.toarray()\n",
    "# creating target classes\n",
    "Y = np.array([])\n",
    "for text in df.id:\n",
    "    Y = np.append(Y, text)\n",
    "# First 1500 for training set, last 500 for test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB()\n",
    "# clf = BernoulliNB()\n",
    "model = clf.fit(X_train, y_train)\n",
    "text_clf_red = Pipeline([('vect', CountVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer)), \n",
    "                       ('reducer', SelectKBest(chi2, k=800)),\n",
    "                       ('clf', MultinomialNB())\n",
    "                       ])\n",
    "model_new = text_clf_red.fit(text_data[:1500],Y[:1500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.55      0.64      0.59        56\n",
      "       10001       0.33      0.25      0.29        36\n",
      "       10002       0.58      0.45      0.51        31\n",
      "       10003       0.32      0.52      0.40        87\n",
      "       10004       0.00      0.00      0.00         2\n",
      "       10005       0.60      0.62      0.61        52\n",
      "       10006       0.40      0.39      0.40        44\n",
      "       10007       0.00      0.00      0.00         2\n",
      "       10008       0.56      0.70      0.62        46\n",
      "       10009       0.00      0.00      0.00         4\n",
      "       10010       0.30      0.27      0.29        11\n",
      "       10011       0.00      0.00      0.00         7\n",
      "       10012       0.50      0.25      0.33         4\n",
      "       10013       0.61      0.46      0.52        37\n",
      "       10014       1.00      0.17      0.29         6\n",
      "       10015       0.60      0.62      0.61        24\n",
      "       10016       0.14      0.07      0.10        14\n",
      "       10017       0.00      0.00      0.00        12\n",
      "       10018       0.40      0.20      0.27        10\n",
      "       10019       0.43      0.20      0.27        15\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       500\n",
      "   macro avg       0.37      0.29      0.30       500\n",
      "weighted avg       0.45      0.46      0.44       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doodey\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.84      0.87      0.85       188\n",
      "       10001       0.81      0.69      0.75       104\n",
      "       10002       0.72      0.82      0.76        99\n",
      "       10003       0.69      0.76      0.72       271\n",
      "       10004       1.00      0.40      0.57        15\n",
      "       10005       0.72      0.82      0.77       142\n",
      "       10006       0.80      0.83      0.81       145\n",
      "       10007       1.00      0.20      0.33         5\n",
      "       10008       0.80      0.85      0.83       117\n",
      "       10009       0.75      0.50      0.60        12\n",
      "       10010       0.58      0.62      0.60        45\n",
      "       10011       0.00      0.00      0.00         6\n",
      "       10012       0.71      0.48      0.57        21\n",
      "       10013       0.81      0.91      0.86        67\n",
      "       10014       1.00      0.39      0.56        23\n",
      "       10015       0.85      0.98      0.91        95\n",
      "       10016       0.79      0.51      0.62        45\n",
      "       10017       0.89      0.49      0.63        35\n",
      "       10018       0.83      0.68      0.75        28\n",
      "       10019       0.96      0.59      0.73        37\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1500\n",
      "   macro avg       0.78      0.62      0.66      1500\n",
      "weighted avg       0.78      0.77      0.76      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doodey\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.62      0.70      0.66        56\n",
      "       10001       0.40      0.22      0.29        36\n",
      "       10002       0.65      0.55      0.60        31\n",
      "       10003       0.30      0.67      0.42        87\n",
      "       10004       0.00      0.00      0.00         2\n",
      "       10005       0.65      0.63      0.64        52\n",
      "       10006       0.54      0.34      0.42        44\n",
      "       10007       0.00      0.00      0.00         2\n",
      "       10008       0.64      0.70      0.67        46\n",
      "       10009       0.00      0.00      0.00         4\n",
      "       10010       0.75      0.27      0.40        11\n",
      "       10011       0.00      0.00      0.00         7\n",
      "       10012       0.50      0.25      0.33         4\n",
      "       10013       0.71      0.41      0.52        37\n",
      "       10014       1.00      0.17      0.29         6\n",
      "       10015       0.68      0.62      0.65        24\n",
      "       10016       0.50      0.21      0.30        14\n",
      "       10017       0.00      0.00      0.00        12\n",
      "       10018       0.50      0.20      0.29        10\n",
      "       10019       0.71      0.33      0.45        15\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       500\n",
      "   macro avg       0.46      0.31      0.35       500\n",
      "weighted avg       0.53      0.49      0.48       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_new.predict(text_data[1500:])\n",
    "print(classification_report(Y[1500:], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.72      0.73      0.73       188\n",
      "       10001       0.69      0.45      0.55       104\n",
      "       10002       0.65      0.70      0.67        99\n",
      "       10003       0.49      0.80      0.61       271\n",
      "       10004       1.00      0.60      0.75        15\n",
      "       10005       0.68      0.72      0.70       142\n",
      "       10006       0.74      0.54      0.63       145\n",
      "       10007       1.00      0.60      0.75         5\n",
      "       10008       0.78      0.78      0.78       117\n",
      "       10009       0.71      0.42      0.53        12\n",
      "       10010       0.94      0.38      0.54        45\n",
      "       10011       1.00      0.50      0.67         6\n",
      "       10012       0.80      0.57      0.67        21\n",
      "       10013       0.81      0.75      0.78        67\n",
      "       10014       1.00      0.74      0.85        23\n",
      "       10015       0.89      0.88      0.89        95\n",
      "       10016       0.79      0.33      0.47        45\n",
      "       10017       0.95      0.54      0.69        35\n",
      "       10018       0.85      0.79      0.81        28\n",
      "       10019       0.87      0.54      0.67        37\n",
      "\n",
      "   micro avg       0.68      0.68      0.68      1500\n",
      "   macro avg       0.82      0.62      0.69      1500\n",
      "weighted avg       0.72      0.68      0.68      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_new.predict(text_data[:1500])\n",
    "print(classification_report(Y[:1500], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
