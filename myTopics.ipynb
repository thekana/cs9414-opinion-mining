{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics, tree\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_selection import chi2, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.tsv', sep='\\t', quoting=csv.QUOTE_NONE, dtype=str, encoding = 'utf-8',\n",
    "                 header=None, names=[\"instance\", \"text\", \"id\", \"sentiment\", \"is_sarcastic\"])\n",
    "#df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.array([])\n",
    "# Read tweets\n",
    "for text in df.text:\n",
    "    text_data = np.append(text_data, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Functions for text pre-processing \"\"\"\n",
    "\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \" \", sample)\n",
    "\n",
    "\n",
    "def remove_punctuation(sample):\n",
    "    \"\"\"Remove punctuations from a sample string\"\"\"\n",
    "#     punctuations = r'''$!\"&'()*+,-./:;<=>?[\\]^`{|}~'''\n",
    "#     no_punct = \"\"\n",
    "#     for char in sample:\n",
    "#         if char not in punctuations:\n",
    "#             no_punct = no_punct + char\n",
    "#     return no_punct\n",
    "    return re.sub(r'[^\\w\\s\\&\\#\\@\\$\\%\\_]','',sample)\n",
    "\n",
    "def myTokenizer(sample):\n",
    "    \"\"\"Customized tokenizer\"\"\"\n",
    "    ################################## 1. Remove numbers\n",
    "    ################################## 2. Remove auspoll thingy\n",
    "    ################################## 3. Remove starts with au\n",
    "    new_words = []\n",
    "    words = sample.split(' ')\n",
    "    new_words = [word for word in words if len(word) >= 2 and not word.isdigit() and not word.startswith('#aus') and not word.startswith('au') and not bool(re.search(r'\\d',word))]\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords_NLTK(sample):\n",
    "    \"\"\"Remove stopwords using NLTK\"\"\"\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    filteredText = \"\"\n",
    "    for word in words:\n",
    "        if word not in stopWords:\n",
    "            filteredText = filteredText + word + \" \"\n",
    "    return filteredText.rstrip()\n",
    "\n",
    "\n",
    "def porter_stem(sample):\n",
    "    \"\"\"Stemming\"\"\"\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + ps.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "def lemmy(sample):\n",
    "    #nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    lemmed_text = \"\"\n",
    "    for word in words:\n",
    "        lemmed_text = lemmed_text + lemmatizer.lemmatize(word) + \" \"\n",
    "    return lemmed_text.rstrip()\n",
    "    \n",
    "def snowball(sample):\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + stemmer.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "def myPreprocessor(sample):\n",
    "    \"\"\"Customized preprocessor\"\"\"\n",
    "    sample = remove_URL(sample)\n",
    "    sample = sample.lower()\n",
    "    sample = remove_stopwords_NLTK(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "#     sample = lemmy(sample)\n",
    "#     sample = snowball(sample)\n",
    "    sample = porter_stem(sample)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myTokenizer('While the majority of all Internet search engines utilize stop words, they do not prevent a user from using them, but they are ignored.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#afpraid', '#agchatoz', '#alp', '#asylumseek', '#betterfutur', '#bootturnbullout', '#brexit', '#cfa', '#cfmeu', '#chafta', '#childcar', '#cleanenergi', '#climat', '#climatechang', '#coal', '#corrupt', '#csg', '#csiro', '#csirocut', '#dutton', '#educ', '#environ', '#et', '#faketradi', '#fraudband', '#gonski', '#greatbarrierreef', '#green', '#health', '#humanright', '#icac', '#inequ', '#insid', '#labor', '#labor posit', '#laborlaunch', '#latelin', '#leadersdeb', '#liber', '#libral', '#lnp', '#lnp #malcolm', '#lnpfail', '#malcolm', '#malwar', '#marriageequ', '#medicar', '#nauru', '#nbn', '#nbn plan', '#nbnco', '#nbngate', '#ndi', '#negativegear', '#npc', '#nswpol', '#panamapap', '#parakeelia', '#peoplesforum', '#pmlive', '#polita', '#putlnplast', '#qanda', '#qldpol', '#refuge', '#renew', '#safeschool', '#savemedicar', '#scomo', '#spendomet', '#springst', '#ssm', '#ssm plebiscit', '#stopstateterror', '#stopstateterror #lnp', '#taxrort', '#tennew', '#thedrum', '#turnbul', '$$$', '$billion', '&amp', '&amp green', '&amp growth', '&amp lnp', '&amp turnbul', '&gt', '@abcnew', '@albomp', '@australianlabor', '@barnaby_joyc', '@billshortenmp', '@billshortenmp say', '@bowenchri', '@cpyne', '@green', '@greghuntmp', '@independentau', '@juliebishopmp', '@liberalau', '@mathiascormann', '@peterdutton_mp', '@richarddinatal', '@scottmorrisonmp', '@skynewsaust', '@smh', '@tanya_plibersek', '@theag', '@tonyabbottmhr', '@tonyhwindsor', '@turnbullmalcolm', '@turnbullmalcolm say', 'aaa', 'aaa credit', 'abbott', 'abc', 'abc sb', 'abl', 'absolut', 'abus', 'account', 'act', 'action', 'actual', 'ad', 'address', 'admit', 'adopt', 'afford', 'afford housingfair', 'afp', 'afp raid', 'again', 'age', 'agenda', 'agent', 'ago', 'agre', 'aid', 'all', 'alleg', 'allianc', 'allow', 'alon', 'alp', 'alp &amp', 'alreadi', 'also', 'alway', 'analysi', 'andrew', 'announc', 'anoth', 'answer', 'anyon', 'anyth', 'approv', 'around', 'ask', 'assist', 'asylum', 'asylum seeker', 'attack', 'avoid', 'away', 'back', 'ban', 'bank', 'barnabi', 'barnabi joyc', 'barrier', 'barrier reef', 'becom', 'behind', 'believ', 'benefit', 'best', 'better', 'beyond', 'big', 'big busi', 'bigger', 'biggest', 'bill', 'bill #blackhol', 'bill shorten', 'billion', 'bishop', 'bit', 'black', 'black hole', 'blame', 'blood', 'bloodi', 'bloodi #refuge', 'blow', 'boat', 'boat &amp', 'border', 'border protect', 'boss', 'bottom', 'bottom line', 'bowen', 'break', 'brexit', 'bribe', 'bring', 'britain', 'broadband', 'broke', 'broken', 'budget', 'build', 'bulk', 'bulk bill', 'bulkbil', 'bullshit', 'busi', 'buy', 'call', 'camp', 'campaign', 'candid', 'cannot', 'cant', 'cant afford', 'cant deliv', 'carbon', 'carbon tax', 'care', 'care educ', 'cash', 'cattl', 'caus', 'cayman', 'cayman island', 'centr', 'ceo', 'cfa', 'cfa volunt', 'chang', 'chang fair', 'chang folk', 'chang pension', 'chao', 'check', 'child', 'childcar', 'childcar support', 'children', 'choic', 'chri', 'chri bowen', 'claim', 'class', 'clean', 'climat', 'climat chang', 'co', 'coal', 'coalit', 'come', 'comment', 'commiss', 'commit', 'commun', 'compani', 'compani tax', 'compass', 'competit', 'concern', 'confid', 'confirm', 'consid', 'consider', 'contact', 'continu', 'copper', 'core', 'cormann', 'corp', 'corp tax', 'corpor', 'corpor tax', 'corrupt', 'cost', 'cost tax', 'could', 'council', 'countri', 'coz', 'creat', 'credibl', 'credit', 'credit rate', 'crimin', 'crisi', 'criticis', 'csiro', 'cut', 'cut abc', 'cut health', 'cut neg', 'cut penalti', 'cut rich', 'dairi', 'dairi farmer', 'damag', 'day', 'deal', 'death', 'debat', 'debt', 'deceiv', 'decid', 'decis', 'declar', 'defend', 'deficit', 'deliv', 'democraci', 'deni', 'dept', 'deserv', 'desper', 'destroy', 'detail', 'detent', 'develop', 'di', 'di natal', 'die', 'differ', 'digit', 'digit economi', 'direct', 'direct action', 'disgrac', 'distract', 'doctor', 'document', 'dodgi', 'dollar', 'donat', 'done', 'donor', 'dont', 'door', 'dr', 'drive', 'dud', 'due', 'dutton', 'earli', 'earner', 'econom', 'econom growth', 'econom manag', 'econom plan', 'economi', 'ed', 'educ', 'educ health', 'educ medicar', 'educ pension', 'effect', 'either', 'elect', 'elector', 'electr', 'end', 'energi', 'english', 'ensur', 'equal', 'estat', 'estat agent', 'et', 'etc', 'eu', 'even', 'ever', 'everi', 'everi time', 'everyon', 'everyth', 'evid', 'exactli', 'expect', 'expens', 'expert', 'explain', 'export', 'expos', 'extra', 'eye', 'face', 'fact', 'fail', 'fair', 'fair tax', 'fair taxat', 'fake', 'fall', 'famili', 'far', 'farmer', 'fault', 'fear', 'feder', 'fee', 'feel', 'fight', 'final', 'financ', 'find', 'first', 'fix', 'flow', 'folk', 'follow', 'fool', 'forc', 'foreign', 'forest', 'forget', 'former', 'found', 'free', 'freez', 'fta', 'full', 'fulli', 'fund', 'fund csiro', 'funnel', 'furiou', 'futur', 'ga', 'gain', 'gay', 'gay marriag', 'gdp', 'gear', 'gener', 'get', 'gfc', 'gillard', 'give', 'give corpor', 'given', 'global', 'go', 'go all', 'goe', 'gone', 'good', 'got', 'gov', 'govern', 'govt', 'gp', 'great', 'great barrier', 'green', 'green &amp', 'greg', 'greg hunt', 'grow', 'growth', 'gst', 'guarante', 'half', 'halt', 'hand', 'happen', 'hard', 'hate', 'haven', 'he', 'head', 'headlin', 'headspac', 'health', 'health &amp', 'health educ', 'healthcar', 'hear', 'heard', 'heart', 'held', 'help', 'here', 'hey', 'hide', 'high', 'higher', 'higher tax', 'histori', 'hit', 'hockey', 'hole', 'home', 'home again', 'homeless', 'hope', 'hopeless', 'hospit', 'hous', 'hous fair', 'hous price', 'housingfair', 'housingfair go', 'human', 'hung', 'hunt', 'hurt', 'hypocrisi', 'id', 'idea', 'ideolog', 'ignor', 'ill', 'illeg', 'illiter', 'illiter innumer', 'illiter refuge', 'im', 'imagin', 'immigr', 'immigr minist', 'impact', 'import', 'in', 'includ', 'increas', 'independ', 'indonesia', 'industri', 'inform', 'infrastructur', 'innov', 'innumer', 'insignific', 'insignific boat', 'instead', 'integr', 'interest', 'intern', 'internet', 'introduc', 'invest', 'investig', 'involv', 'iron', 'is', 'islam', 'islamist', 'island', 'isnt', 'issu', 'it', 'job', 'job &amp', 'job growth', 'john', 'joke', 'joyc', 'joyc link', 'juli', 'juli bishop', 'justifi', 'keep', 'kelli', 'kid', 'kill', 'knew', 'knew #lnpfail', 'know', 'know lnp', 'known', 'labor', 'labor commit', 'labor fund', 'labor govern', 'labor govt', 'labor neg', 'labor parti', 'labor plan', 'labor polici', 'labor posit', 'labor put', 'labor seek', 'labor spend', 'labor target', 'labor want', 'labor would', 'lack', 'last', 'last time', 'launder', 'law', 'lead', 'lead higher', 'leader', 'leader bill', 'leak', 'learn', 'least', 'leav', 'left', 'legal', 'less', 'let', 'level', 'liar', 'lib', 'lib parti', 'liber', 'liber candid', 'liber cut', 'liber last', 'liber parti', 'lie', 'lie medicar', 'like', 'limit', 'line', 'link', 'listen', 'littl', 'live', 'live cattl', 'live export', 'lnp', 'lnp cut', 'lnp destroy', 'lnp govt', 'lnp last', 'lnp want', 'lnp win', 'loan', 'local', 'lock', 'lock away', 'lol', 'long', 'look', 'lose', 'lost', 'lot', 'love', 'low', 'lower', 'mad', 'made', 'mafia', 'major', 'make', 'mal', 'malcolm', 'malcolm turnbul', 'man', 'manag', 'mani', 'manu', 'manufactur', 'margin', 'margin seat', 'mark', 'market', 'marriag', 'marriag equal', 'marriag plebiscit', 'mate', 'mathia', 'may', 'me', 'mean', 'measur', 'media', 'medic', 'medicar', 'medicar tafe', 'member', 'mental', 'mention', 'migrant', 'mill', 'million', 'min', 'mine', 'minist', 'minor', 'model', 'money', 'money launder', 'morn', 'morrison', 'mossack', 'mp', 'mr', 'mr turnbul', 'msm', 'mt', 'much', 'multin', 'murdoch', 'muslim', 'must', 'name', 'natal', 'nation', 'nauru', 'nbn', 'need', 'neg', 'neg gear', 'never', 'new', 'news', 'next', 'next term', 'nigel', 'no', 'not', 'noth', 'now', 'nsw', 'obsess', 'off', 'offer', 'offic', 'offici', 'offshor', 'ok', 'old', 'on', 'one', 'open', 'opportun', 'opposit', 'order', 'otherwis', 'out', 'outcom', 'outsourc', 'oversea', 'owner', 'oz', 'pa', 'packag', 'paid', 'panama', 'panama paper', 'paper', 'parakeelia', 'parent', 'parliament', 'part', 'parti', 'pass', 'past', 'patient', 'pay', 'pay tax', 'payment', 'penalti', 'penalti rate', 'pension', 'pension abc', 'peopl', 'per', 'person', 'perth', 'peter', 'peter dutton', 'photo', 'piti', 'pl', 'plan', 'plan deliv', 'plan educ', 'plan job', 'pleas', 'plebiscit', 'pledg', 'pm', 'pm @turnbullmalcolm', 'pm turnbul', 'pocket', 'point', 'polic', 'polici', 'polit', 'polit parti', 'politician', 'poll', 'poor', 'posit', 'posit plan', 'power', 'ppl', 'price', 'prime', 'prime minist', 'privat', 'privatis', 'privatis #medicar', 'privatis medicar', 'problem', 'profit', 'program', 'promis', 'promot', 'properti', 'protect', 'protest', 'prove', 'provid', 'public', 'public transport', 'pull', 'put', 'put liber', 'put lnp', 'pyne', 'question', 'quit', 'racism', 'radic', 'raid', 'raid alp', 'rais', 'rate', 'rate #nbn', 'rba', 're', 'read', 'readi', 'real', 'realiti', 'realli', 'reason', 'rebat', 'recess', 'record', 'reduc', 'reef', 'reform', 'refuge', 'refuge take', 'refus', 'releas', 'rememb', 'remov', 'renew', 'renew energi', 'report', 'research', 'respect', 'respond', 'respons', 'restor', 'return', 'reveal', 'revenu', 'revers', 'review', 'rhetor', 'rich', 'right', 'rip', 'rise', 'risk', 'road', 'roll', 'rort', 'royal', 'royal commiss', 'rt', 'rule', 'run', 'sa', 'sack', 'safe', 'said', 'sale', 'samesex', 'samesex marriag', 'save', 'save medicar', 'say', 'say @richarddinatal', 'say he', 'say labor', 'sb', 'scandal', 'scare', 'scare campaign', 'school', 'scott', 'scott morrison', 'scrap', 'screw', 'sea', 'seat', 'second', 'second rate', 'secret', 'sector', 'secur', 'see', 'seek', 'seeker', 'seem', 'sell', 'senat', 'seriou', 'servic', 'set', 'shame', 'share', 'shift', 'shit', 'shock', 'shoot', 'shorten', 'shorten say', 'show', 'shut', 'sign', 'sinc', 'singl', 'slash', 'slow', 'sm', 'small', 'small busi', 'so', 'social', 'societi', 'solut', 'someth', 'soon', 'sort', 'south', 'spend', 'spent', 'ssm', 'stabil', 'stand', 'start', 'start again', 'state', 'statement', 'still', 'stop', 'stop boat', 'stori', 'strategi', 'strong', 'strong economi', 'student', 'stuff', 'stupid', 'submarin', 'subsidi', 'suggest', 'suicid', 'super', 'superannu', 'support', 'sure', 'surplu', 'surpris', 'surviv', 'sydney', 'system', 'system afford', 'tafe', 'tafe climat', 'take', 'take job', 'talk', 'talk #parakeelia', 'talk health', 'target', 'tax', 'tax avoid', 'tax benefit', 'tax cut', 'tax haven', 'tax hurt', 'tax rate', 'tax system', 'taxat', 'taxpay', 'taxpay money', 'team', 'tell', 'ten', 'term', 'terror', 'terrorist', 'test', 'thank', 'that', 'the', 'them', 'there', 'therel', 'theyll', 'theyr', 'thi', 'thing', 'think', 'though', 'thought', 'thousand', 'threat', 'till', 'time', 'time cut', 'today', 'told', 'toni', 'toni abbott', 'too', 'took', 'top', 'total', 'trade', 'transfer', 'transit', 'transport', 'treasur', 'treasuri', 'tri', 'trickl', 'true', 'trump', 'trust', 'trust medicar', 'trustpm', 'trustpm shonki', 'truth', 'turn', 'turnbul', 'turnbul &amp', 'turnbul absolut', 'turnbul govern', 'turnbul govt', 'turnbul job', 'turnbul plan', 'turnbul say', 'turnbul tax', 'turnbul want', 'two', 'uk', 'unemploy', 'uni', 'union', 'urg', 'us', 'use', 'usual', 'valu', 'via', 'via @abcnew', 'via @smh', 'via @theag', 'victoria', 'view', 'visa', 'visit', 'volunt', 'vote', 'vote #lnp', 'vote labor', 'vote liber', 'vote lnp', 'voter', 'vulner', 'wa', 'wage', 'wait', 'wake', 'want', 'want cut', 'want know', 'war', 'warn', 'wast', 'watch', 'water', 'way', 'we', 'wealthi', 'week', 'welfar', 'well', 'were', 'weve', 'what', 'whether', 'who', 'who trustpm', 'whole', 'win', 'windsor', 'within', 'without', 'women', 'wonder', 'wont', 'word', 'work', 'worker', 'world', 'wors', 'worst', 'worth', 'would', 'wrong', 'xenophobia', 'ya', 'ye', 'yeah', 'year', 'yet', 'you', 'young', 'your', 'yr', 'zero']\n",
      "1137\n"
     ]
    }
   ],
   "source": [
    "# try to use sklearn stop_words later\n",
    "# 711, 0.688\n",
    "# 1178, 0.978\n",
    "count = CountVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer, max_features=1137, ngram_range=(1, 2), min_df = 0)\n",
    "# count = TfidfVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer, max_features=800,sublinear_tf=True, min_df=5,\n",
    "#                         ngram_range=(1, 2), \n",
    "#                         stop_words='english')\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "print(count.get_feature_names())\n",
    "# size = len(count.vocabulary_)\n",
    "print(len(count.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_words.toarray()\n",
    "# creating target classes\n",
    "Y = np.array([])\n",
    "for text in df.id:\n",
    "    Y = np.append(Y, text)\n",
    "# First 1500 for training set, last 500 for test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha = 1.0, fit_prior = True)\n",
    "#clf = BernoulliNB(alpha = 1.0, fit_prior = False)\n",
    "model = clf.fit(X_train, y_train)\n",
    "#model = clf.fit(X, Y)\n",
    "#print(model.class_log_prior_ )\n",
    "# text_clf_red = Pipeline([('vect', CountVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer)), \n",
    "#                        ('reducer', SelectKBest(chi2, k=200)),\n",
    "#                        ('clf', MultinomialNB())\n",
    "#                        ])\n",
    "# model_new = text_clf_red.fit(text_data[:1500],Y[:1500])\n",
    "# model_new = text_clf_red.fit(text_data,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.60      0.73      0.66        56\n",
      "       10001       0.36      0.25      0.30        36\n",
      "       10002       0.62      0.52      0.56        31\n",
      "       10003       0.33      0.53      0.41        87\n",
      "       10004       0.00      0.00      0.00         2\n",
      "       10005       0.61      0.63      0.62        52\n",
      "       10006       0.39      0.36      0.38        44\n",
      "       10007       0.00      0.00      0.00         2\n",
      "       10008       0.56      0.67      0.61        46\n",
      "       10009       0.00      0.00      0.00         4\n",
      "       10010       0.14      0.09      0.11        11\n",
      "       10011       0.00      0.00      0.00         7\n",
      "       10012       0.00      0.00      0.00         4\n",
      "       10013       0.67      0.49      0.56        37\n",
      "       10014       0.67      0.33      0.44         6\n",
      "       10015       0.54      0.62      0.58        24\n",
      "       10016       0.17      0.07      0.10        14\n",
      "       10017       0.00      0.00      0.00        12\n",
      "       10018       0.50      0.30      0.37        10\n",
      "       10019       0.38      0.20      0.26        15\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       500\n",
      "   macro avg       0.33      0.29      0.30       500\n",
      "weighted avg       0.45      0.47      0.45       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\King\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.82      0.86      0.84       188\n",
      "       10001       0.77      0.66      0.71       104\n",
      "       10002       0.71      0.81      0.76        99\n",
      "       10003       0.69      0.76      0.73       271\n",
      "       10004       1.00      0.47      0.64        15\n",
      "       10005       0.73      0.83      0.78       142\n",
      "       10006       0.79      0.83      0.81       145\n",
      "       10007       1.00      0.20      0.33         5\n",
      "       10008       0.78      0.85      0.82       117\n",
      "       10009       0.75      0.50      0.60        12\n",
      "       10010       0.56      0.62      0.59        45\n",
      "       10011       0.00      0.00      0.00         6\n",
      "       10012       0.69      0.43      0.53        21\n",
      "       10013       0.84      0.88      0.86        67\n",
      "       10014       1.00      0.39      0.56        23\n",
      "       10015       0.83      0.95      0.88        95\n",
      "       10016       0.76      0.49      0.59        45\n",
      "       10017       0.93      0.40      0.56        35\n",
      "       10018       0.73      0.68      0.70        28\n",
      "       10019       0.88      0.57      0.69        37\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1500\n",
      "   macro avg       0.76      0.61      0.65      1500\n",
      "weighted avg       0.77      0.76      0.75      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\King\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_new.predict(text_data[1500:])\n",
    "# print(classification_report(Y[1500:], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_new.predict(text_data[:1500])\n",
    "# print(classification_report(Y[:1500], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
