{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics, tree\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_selection import chi2, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.tsv', sep='\\t', quoting=csv.QUOTE_NONE, dtype=str, encoding = 'utf-8',\n",
    "                 header=None, names=[\"instance\", \"text\", \"id\", \"sentiment\", \"is_sarcastic\"])\n",
    "#df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.array([])\n",
    "# Read tweets\n",
    "for text in df.text:\n",
    "    text_data = np.append(text_data, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Functions for text pre-processing \"\"\"\n",
    "\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \" \", sample)\n",
    "\n",
    "\n",
    "def remove_punctuation(sample):\n",
    "    \"\"\"Remove punctuations from a sample string\"\"\"\n",
    "#     punctuations = r'''$!\"&'()*+,-./:;<=>?[\\]^`{|}~'''\n",
    "#     no_punct = \"\"\n",
    "#     for char in sample:\n",
    "#         if char not in punctuations:\n",
    "#             no_punct = no_punct + char\n",
    "#     return no_punct\n",
    "    return re.sub(r'[^\\w\\s\\&\\#\\@\\$\\%\\_]','',sample)\n",
    "\n",
    "def myTokenizer(sample):\n",
    "    \"\"\"Customized tokenizer\"\"\"\n",
    "    ################################## 1. Remove numbers\n",
    "    ################################## 2. Remove auspoll thingy\n",
    "    ################################## 3. Remove starts with au\n",
    "    new_words = []\n",
    "    words = sample.split(' ')\n",
    "    new_words = [word for word in words if len(word) >= 2 and not word.isdigit() and not word.startswith('#aus') and not word.startswith('au') and not bool(re.search(r'\\d',word))]\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords_NLTK(sample):\n",
    "    \"\"\"Remove stopwords using NLTK\"\"\"\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    filteredText = \"\"\n",
    "    for word in words:\n",
    "        if word not in stopWords:\n",
    "            filteredText = filteredText + word + \" \"\n",
    "    return filteredText.rstrip()\n",
    "\n",
    "\n",
    "def porter_stem(sample):\n",
    "    \"\"\"Stemming\"\"\"\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + ps.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "def lemmy(sample):\n",
    "    #nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    lemmed_text = \"\"\n",
    "    for word in words:\n",
    "        lemmed_text = lemmed_text + lemmatizer.lemmatize(word) + \" \"\n",
    "    return lemmed_text.rstrip()\n",
    "    \n",
    "def snowball(sample):\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + stemmer.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "def myPreprocessor(sample):\n",
    "    \"\"\"Customized preprocessor\"\"\"\n",
    "    sample = remove_URL(sample)\n",
    "    sample = sample.lower()\n",
    "    sample = remove_stopwords_NLTK(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "#     sample = lemmy(sample)\n",
    "#     sample = snowball(sample)\n",
    "    sample = porter_stem(sample)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myTokenizer('While the majority of all Internet search engines utilize stop words, they do not prevent a user from using them, but they are ignored.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#afpraid', '#alp', '#asylumseek', '#brexit', '#cfa', '#cfmeu', '#chafta', '#climat', '#climatechang', '#coal', '#corrupt', '#csg', '#csiro', '#csirocut', '#dutton', '#educ', '#et', '#faketradi', '#fraudband', '#gonski', '#green', '#humanright', '#icac', '#inequ', '#insid', '#labor', '#laborlaunch', '#leadersdeb', '#liber', '#lnp', '#lnp #malcolm', '#lnpfail', '#malcolm', '#marriageequ', '#medicar', '#nauru', '#nbn', '#nbnco', '#nbngate', '#negativegear', '#npc', '#panamapap', '#parakeelia', '#peoplesforum', '#putlnplast', '#qanda', '#qldpol', '#refuge', '#renew', '#savemedicar', '#scomo', '#spendomet', '#springst', '#ssm', '#stopstateterror', '#stopstateterror #lnp', '#tennew', '#thedrum', '#turnbul', '$$$', '$billion', '&amp', '&amp green', '&amp growth', '&amp lnp', '&amp turnbul', '@abcnew', '@albomp', '@australianlabor', '@barnaby_joyc', '@billshortenmp', '@billshortenmp say', '@bowenchri', '@cpyne', '@green', '@greghuntmp', '@juliebishopmp', '@liberalau', '@mathiascormann', '@peterdutton_mp', '@richarddinatal', '@scottmorrisonmp', '@smh', '@tanya_plibersek', '@theag', '@tonyabbottmhr', '@turnbullmalcolm', '@turnbullmalcolm say', 'aaa', 'abbott', 'abc', 'abl', 'absolut', 'abus', 'action', 'actual', 'ad', 'admit', 'adopt', 'afford', 'afp', 'afp raid', 'again', 'age', 'agenda', 'agre', 'all', 'allow', 'alp', 'alreadi', 'also', 'alway', 'announc', 'anoth', 'answer', 'anyon', 'anyth', 'around', 'ask', 'asylum', 'asylum seeker', 'attack', 'avoid', 'away', 'back', 'ban', 'bank', 'barnabi', 'barnabi joyc', 'becom', 'behind', 'believ', 'benefit', 'best', 'better', 'big', 'big busi', 'biggest', 'bill', 'bill shorten', 'bill shorten say', 'billion', 'bishop', 'black', 'black hole', 'blame', 'boat', 'border', 'border protect', 'bottom', 'bowen', 'break', 'brexit', 'broadband', 'broken', 'budget', 'build', 'bulk', 'bulk bill', 'busi', 'buy', 'call', 'campaign', 'candid', 'cannot', 'cant', 'cant afford', 'carbon', 'carbon tax', 'care', 'cash', 'cattl', 'caus', 'cayman', 'centr', 'ceo', 'cfa', 'chang', 'child', 'childcar', 'children', 'choic', 'chri', 'claim', 'class', 'clean', 'climat', 'climat chang', 'co', 'coal', 'coalit', 'come', 'comment', 'commiss', 'commit', 'commun', 'compani', 'concern', 'confid', 'confirm', 'contact', 'continu', 'copper', 'cormann', 'corp', 'corp tax', 'corpor', 'corpor tax', 'corpor tax cut', 'corrupt', 'cost', 'could', 'council', 'countri', 'coz', 'creat', 'credibl', 'credit', 'crimin', 'csiro', 'cut', 'cut health', 'cut health educ', 'cut penalti', 'cut penalti rate', 'dairi', 'dairi farmer', 'day', 'deal', 'debat', 'debt', 'decid', 'decis', 'declar', 'defend', 'deficit', 'deliv', 'democraci', 'deni', 'dept', 'deserv', 'desper', 'destroy', 'detail', 'detent', 'develop', 'di', 'di natal', 'die', 'differ', 'digit', 'direct', 'disgrac', 'distract', 'doctor', 'document', 'dodgi', 'dollar', 'donat', 'done', 'donor', 'dont', 'due', 'dutton', 'earli', 'econom', 'econom manag', 'econom plan', 'economi', 'educ', 'educ medicar', 'effect', 'either', 'elect', 'elector', 'electr', 'end', 'energi', 'english', 'ensur', 'equal', 'etc', 'even', 'ever', 'everi', 'everi time', 'everyth', 'evid', 'expect', 'expens', 'explain', 'export', 'expos', 'extra', 'eye', 'fact', 'fail', 'fair', 'fall', 'famili', 'far', 'farmer', 'fear', 'feder', 'fight', 'final', 'financ', 'find', 'first', 'fix', 'forc', 'foreign', 'forget', 'former', 'free', 'freez', 'full', 'fund', 'funnel', 'futur', 'gay', 'gear', 'get', 'gfc', 'give', 'given', 'go', 'goe', 'good', 'got', 'gov', 'govern', 'govt', 'gp', 'great', 'green', 'greg', 'greg hunt', 'grow', 'growth', 'gst', 'guarante', 'half', 'happen', 'hard', 'hate', 'haven', 'he', 'head', 'health', 'health educ', 'hear', 'held', 'help', 'here', 'hey', 'hide', 'high', 'higher', 'higher tax', 'histori', 'hit', 'hole', 'home', 'hope', 'hospit', 'hous', 'hunt', 'hurt', 'idea', 'ignor', 'illeg', 'illiter', 'im', 'imagin', 'immigr', 'impact', 'import', 'in', 'increas', 'independ', 'indonesia', 'industri', 'innov', 'integr', 'interest', 'internet', 'invest', 'investig', 'is', 'islam', 'island', 'isnt', 'issu', 'it', 'job', 'job &amp', 'job &amp growth', 'job growth', 'joke', 'joyc', 'juli', 'juli bishop', 'keep', 'kelli', 'kid', 'kill', 'knew', 'know', 'known', 'labor', 'labor govern', 'labor neg', 'labor neg gear', 'labor parti', 'labor want', 'lack', 'last', 'launder', 'law', 'lead', 'leader', 'leak', 'learn', 'left', 'legal', 'less', 'let', 'liar', 'lib', 'liber', 'liber cut', 'liber last', 'liber parti', 'lie', 'like', 'line', 'link', 'listen', 'littl', 'live', 'lnp', 'lnp govt', 'lnp last', 'lnp want', 'local', 'long', 'look', 'lose', 'lot', 'love', 'low', 'lower', 'made', 'major', 'make', 'mal', 'malcolm', 'malcolm turnbul', 'man', 'manag', 'mani', 'manu', 'manufactur', 'market', 'marriag', 'marriag equal', 'marriag plebiscit', 'mate', 'may', 'mean', 'measur', 'media', 'medicar', 'member', 'mental', 'mention', 'million', 'mine', 'minist', 'money', 'money launder', 'morrison', 'mp', 'mr', 'mr turnbul', 'msm', 'mt', 'much', 'muslim', 'must', 'name', 'natal', 'nation', 'nauru', 'nbn', 'need', 'neg', 'neg gear', 'never', 'new', 'news', 'next', 'no', 'noth', 'now', 'off', 'offer', 'offic', 'offici', 'offshor', 'ok', 'old', 'on', 'one', 'open', 'opposit', 'order', 'out', 'oversea', 'owner', 'oz', 'paid', 'panama', 'panama paper', 'paper', 'parakeelia', 'parent', 'part', 'parti', 'pass', 'past', 'patient', 'pay', 'pay tax', 'payment', 'penalti', 'penalti rate', 'pension', 'peopl', 'per', 'person', 'peter', 'peter dutton', 'plan', 'plan deliv', 'pleas', 'plebiscit', 'pledg', 'pm', 'pm @turnbullmalcolm', 'pm turnbul', 'point', 'polic', 'polici', 'polit', 'politician', 'poll', 'poor', 'posit', 'posit plan', 'power', 'ppl', 'price', 'prime', 'prime minist', 'privat', 'privatis', 'privatis #medicar', 'privatis medicar', 'problem', 'profit', 'program', 'promis', 'promot', 'properti', 'protect', 'prove', 'provid', 'public', 'put', 'put liber', 'put liber last', 'put lnp', 'put lnp last', 'pyne', 'question', 'quit', 'raid', 'rais', 'rate', 'rba', 'read', 'real', 'realiti', 'realli', 'reason', 'rebat', 'record', 'reduc', 'reef', 'reform', 'refuge', 'refus', 'releas', 'rememb', 'remov', 'renew', 'renew energi', 'report', 'research', 'respons', 'return', 'reveal', 'revers', 'rich', 'right', 'rise', 'risk', 'road', 'rort', 'royal', 'royal commiss', 'run', 'sack', 'safe', 'said', 'samesex', 'samesex marriag', 'save', 'say', 'say @richarddinatal', 'say labor', 'sb', 'scandal', 'scare', 'scare campaign', 'school', 'scott', 'scott morrison', 'scrap', 'screw', 'sea', 'seat', 'second', 'secret', 'sector', 'secur', 'see', 'seek', 'seeker', 'seem', 'sell', 'seriou', 'servic', 'set', 'shame', 'shock', 'shorten', 'shorten say', 'show', 'sign', 'sinc', 'singl', 'slash', 'slow', 'small', 'so', 'social', 'someth', 'soon', 'spend', 'spent', 'ssm', 'stand', 'start', 'state', 'still', 'stop', 'stop boat', 'stori', 'strategi', 'strong', 'super', 'superannu', 'support', 'sure', 'system', 'tafe', 'take', 'take job', 'talk', 'target', 'tax', 'tax cut', 'tax rate', 'tax system', 'taxat', 'taxpay', 'taxpay money', 'team', 'tell', 'ten', 'term', 'terror', 'terrorist', 'test', 'thank', 'that', 'the', 'them', 'there', 'theyll', 'theyr', 'thi', 'thing', 'think', 'though', 'till', 'time', 'today', 'told', 'toni', 'toni abbott', 'took', 'top', 'total', 'trade', 'transfer', 'transit', 'treasur', 'tri', 'true', 'trump', 'trust', 'truth', 'turn', 'turnbul', 'turnbul &amp', 'turnbul govern', 'turnbul govt', 'turnbul job', 'turnbul plan', 'turnbul say', 'turnbul want', 'two', 'uk', 'unemploy', 'union', 'urg', 'us', 'use', 'valu', 'via', 'via @abcnew', 'via @smh', 'via @theag', 'victoria', 'view', 'visa', 'visit', 'volunt', 'vote', 'vote labor', 'vote liber', 'vote lnp', 'voter', 'wa', 'wage', 'wait', 'want', 'want know', 'war', 'warn', 'wast', 'watch', 'water', 'way', 'we', 'wealthi', 'week', 'welfar', 'well', 'were', 'weve', 'what', 'whether', 'who', 'win', 'within', 'without', 'women', 'wonder', 'wont', 'word', 'work', 'worker', 'world', 'wors', 'worst', 'worth', 'would', 'wrong', 'xenophobia', 'ye', 'yeah', 'year', 'yet', 'you', 'your', 'yr', 'zero']\n",
      "812\n"
     ]
    }
   ],
   "source": [
    "# try to use sklearn stop_words later\n",
    "# 711, 0.688\n",
    "# 1178, 0.978\n",
    "count = CountVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer, max_features=1846, ngram_range=(1, 4), min_df = 6)\n",
    "# count = TfidfVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer, max_features=800,sublinear_tf=True, min_df=5,\n",
    "#                         ngram_range=(1, 2), \n",
    "#                         stop_words='english')\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "print(count.get_feature_names())\n",
    "# size = len(count.vocabulary_)\n",
    "print(len(count.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_words.toarray()\n",
    "# creating target classes\n",
    "Y = np.array([])\n",
    "for text in df.id:\n",
    "    Y = np.append(Y, text)\n",
    "# First 1500 for training set, last 500 for test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha = 1.0, fit_prior = True)\n",
    "#clf = BernoulliNB(alpha = 0.9, fit_prior = False)\n",
    "model = clf.fit(X_train, y_train)\n",
    "#model = clf.fit(X, Y)\n",
    "#print(model.class_log_prior_ )\n",
    "# text_clf_red = Pipeline([('vect', CountVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer)), \n",
    "#                        ('reducer', SelectKBest(chi2, k=200)),\n",
    "#                        ('clf', MultinomialNB())\n",
    "#                        ])\n",
    "# model_new = text_clf_red.fit(text_data[:1500],Y[:1500])\n",
    "# model_new = text_clf_red.fit(text_data,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.57      0.68      0.62        56\n",
      "       10001       0.43      0.33      0.38        36\n",
      "       10002       0.60      0.58      0.59        31\n",
      "       10003       0.34      0.52      0.41        87\n",
      "       10004       0.00      0.00      0.00         2\n",
      "       10005       0.61      0.63      0.62        52\n",
      "       10006       0.40      0.39      0.39        44\n",
      "       10007       0.00      0.00      0.00         2\n",
      "       10008       0.58      0.67      0.63        46\n",
      "       10009       0.00      0.00      0.00         4\n",
      "       10010       0.33      0.36      0.35        11\n",
      "       10011       0.00      0.00      0.00         7\n",
      "       10012       0.00      0.00      0.00         4\n",
      "       10013       0.59      0.43      0.50        37\n",
      "       10014       0.00      0.00      0.00         6\n",
      "       10015       0.58      0.62      0.60        24\n",
      "       10016       0.29      0.14      0.19        14\n",
      "       10017       0.00      0.00      0.00        12\n",
      "       10018       0.50      0.30      0.37        10\n",
      "       10019       0.43      0.20      0.27        15\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       500\n",
      "   macro avg       0.31      0.29      0.30       500\n",
      "weighted avg       0.45      0.47      0.46       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\King\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.78      0.81      0.79       188\n",
      "       10001       0.75      0.66      0.70       104\n",
      "       10002       0.70      0.80      0.75        99\n",
      "       10003       0.67      0.74      0.70       271\n",
      "       10004       1.00      0.47      0.64        15\n",
      "       10005       0.71      0.77      0.74       142\n",
      "       10006       0.75      0.78      0.76       145\n",
      "       10007       0.00      0.00      0.00         5\n",
      "       10008       0.78      0.85      0.81       117\n",
      "       10009       0.78      0.58      0.67        12\n",
      "       10010       0.57      0.67      0.61        45\n",
      "       10011       0.00      0.00      0.00         6\n",
      "       10012       0.71      0.57      0.63        21\n",
      "       10013       0.81      0.84      0.82        67\n",
      "       10014       1.00      0.30      0.47        23\n",
      "       10015       0.82      0.95      0.88        95\n",
      "       10016       0.73      0.53      0.62        45\n",
      "       10017       0.87      0.37      0.52        35\n",
      "       10018       0.72      0.64      0.68        28\n",
      "       10019       0.87      0.54      0.67        37\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      1500\n",
      "   macro avg       0.70      0.59      0.62      1500\n",
      "weighted avg       0.74      0.74      0.73      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\King\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_new.predict(text_data[1500:])\n",
    "# print(classification_report(Y[1500:], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_new.predict(text_data[:1500])\n",
    "# print(classification_report(Y[:1500], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
