{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics, tree\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_selection import chi2, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.tsv', sep='\\t', quoting=csv.QUOTE_NONE, dtype=str, encoding = 'utf-8',\n",
    "                 header=None, names=[\"instance\", \"text\", \"id\", \"sentiment\", \"is_sarcastic\"])\n",
    "#df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.array([])\n",
    "# Read tweets\n",
    "for text in df.text:\n",
    "    text_data = np.append(text_data, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Functions for text pre-processing \"\"\"\n",
    "\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \" \", sample)\n",
    "\n",
    "\n",
    "def remove_punctuation(sample):\n",
    "    \"\"\"Remove punctuations from a sample string\"\"\"\n",
    "#     punctuations = r'''$!\"&'()*+,-./:;<=>?[\\]^`{|}~'''\n",
    "#     no_punct = \"\"\n",
    "#     for char in sample:\n",
    "#         if char not in punctuations:\n",
    "#             no_punct = no_punct + char\n",
    "#     return no_punct\n",
    "    return re.sub(r'[^\\w\\s\\&\\#\\@\\$\\%\\_]','',sample)\n",
    "\n",
    "def myTokenizer(sample):\n",
    "    \"\"\"Customized tokenizer\"\"\"\n",
    "    ################################## 1. Remove numbers\n",
    "    ################################## 2. Remove auspoll thingy\n",
    "    ################################## 3. Remove starts with au\n",
    "    new_words = []\n",
    "    words = sample.split(' ')\n",
    "    new_words = [word for word in words if len(word) >= 2 and not word.isdigit() and not word.startswith('#aus') and not word.startswith('au') and not bool(re.search(r'\\d',word))]\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords_NLTK(sample):\n",
    "    \"\"\"Remove stopwords using NLTK\"\"\"\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    filteredText = \"\"\n",
    "    for word in words:\n",
    "        if word not in stopWords:\n",
    "            filteredText = filteredText + word + \" \"\n",
    "    return filteredText.rstrip()\n",
    "\n",
    "\n",
    "def porter_stem(sample):\n",
    "    \"\"\"Stemming\"\"\"\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + ps.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "def lemmy(sample):\n",
    "    #nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    lemmed_text = \"\"\n",
    "    for word in words:\n",
    "        lemmed_text = lemmed_text + lemmatizer.lemmatize(word) + \" \"\n",
    "    return lemmed_text.rstrip()\n",
    "    \n",
    "def snowball(sample):\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + stemmer.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "def myPreprocessor(sample):\n",
    "    \"\"\"Customized preprocessor\"\"\"\n",
    "    sample = remove_URL(sample)\n",
    "    sample = sample.lower()\n",
    "    sample = remove_stopwords_NLTK(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "#     sample = lemmy(sample)\n",
    "#     sample = snowball(sample)\n",
    "    sample = porter_stem(sample)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myTokenizer('While the majority of all Internet search engines utilize stop words, they do not prevent a user from using them, but they are ignored.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1200\n"
     ]
    }
   ],
   "source": [
    "# try to use sklearn stop_words later\n",
    "count = CountVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer, max_features=1200, ngram_range=(1, 2))\n",
    "# count = TfidfVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer, max_features=800,sublinear_tf=True, min_df=5,\n",
    "#                         ngram_range=(1, 2), \n",
    "#                         stop_words='english')\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "# print(count.get_feature_names())\n",
    "# size = len(count.vocabulary_)\n",
    "print(len(count.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_words.toarray()\n",
    "# creating target classes\n",
    "Y = np.array([])\n",
    "for text in df.id:\n",
    "    Y = np.append(Y, text)\n",
    "# First 1500 for training set, last 500 for test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, Y, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.07677842 -2.66882949 -2.71810054 -1.71110157 -4.60517019 -2.35739333\n",
      " -2.33648664 -5.70378247 -2.55104645 -4.82831374 -3.5065579  -5.52146092\n",
      " -4.26869795 -3.10852777 -4.17772617 -2.7593435  -3.5065579  -3.75787233\n",
      " -3.98101588 -3.70230247]\n"
     ]
    }
   ],
   "source": [
    "clf = MultinomialNB(alpha = 1)\n",
    "model = clf.fit(X_train, y_train)\n",
    "#model = clf.fit(X, Y)\n",
    "print(model.class_log_prior_ )\n",
    "text_clf_red = Pipeline([('vect', CountVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer)), \n",
    "                       ('reducer', SelectKBest(chi2, k=200)),\n",
    "                       ('clf', MultinomialNB())\n",
    "                       ])\n",
    "model_new = text_clf_red.fit(text_data[:1500],Y[:1500])\n",
    "# model_new = text_clf_red.fit(text_data,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.59      0.71      0.65        56\n",
      "       10001       0.36      0.25      0.30        36\n",
      "       10002       0.58      0.48      0.53        31\n",
      "       10003       0.33      0.53      0.41        87\n",
      "       10004       0.00      0.00      0.00         2\n",
      "       10005       0.59      0.62      0.60        52\n",
      "       10006       0.42      0.41      0.41        44\n",
      "       10007       0.00      0.00      0.00         2\n",
      "       10008       0.56      0.63      0.59        46\n",
      "       10009       0.00      0.00      0.00         4\n",
      "       10010       0.14      0.09      0.11        11\n",
      "       10011       0.00      0.00      0.00         7\n",
      "       10012       0.00      0.00      0.00         4\n",
      "       10013       0.69      0.49      0.57        37\n",
      "       10014       0.67      0.33      0.44         6\n",
      "       10015       0.54      0.62      0.58        24\n",
      "       10016       0.17      0.07      0.10        14\n",
      "       10017       0.00      0.00      0.00        12\n",
      "       10018       0.57      0.40      0.47        10\n",
      "       10019       0.38      0.20      0.26        15\n",
      "\n",
      "   micro avg       0.47      0.47      0.47       500\n",
      "   macro avg       0.33      0.29      0.30       500\n",
      "weighted avg       0.45      0.47      0.45       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\King\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.83      0.86      0.84       188\n",
      "       10001       0.80      0.67      0.73       104\n",
      "       10002       0.71      0.81      0.76        99\n",
      "       10003       0.69      0.76      0.73       271\n",
      "       10004       1.00      0.47      0.64        15\n",
      "       10005       0.74      0.83      0.78       142\n",
      "       10006       0.80      0.83      0.81       145\n",
      "       10007       1.00      0.20      0.33         5\n",
      "       10008       0.79      0.86      0.82       117\n",
      "       10009       0.75      0.50      0.60        12\n",
      "       10010       0.56      0.62      0.59        45\n",
      "       10011       0.00      0.00      0.00         6\n",
      "       10012       0.69      0.43      0.53        21\n",
      "       10013       0.85      0.90      0.87        67\n",
      "       10014       1.00      0.48      0.65        23\n",
      "       10015       0.83      0.96      0.89        95\n",
      "       10016       0.78      0.47      0.58        45\n",
      "       10017       0.95      0.51      0.67        35\n",
      "       10018       0.74      0.71      0.73        28\n",
      "       10019       0.91      0.57      0.70        37\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      1500\n",
      "   macro avg       0.77      0.62      0.66      1500\n",
      "weighted avg       0.77      0.77      0.76      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\King\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.71      0.36      0.48        56\n",
      "       10001       0.53      0.22      0.31        36\n",
      "       10002       0.56      0.61      0.58        31\n",
      "       10003       0.24      0.71      0.36        87\n",
      "       10004       0.00      0.00      0.00         2\n",
      "       10005       0.79      0.65      0.72        52\n",
      "       10006       0.57      0.18      0.28        44\n",
      "       10007       0.00      0.00      0.00         2\n",
      "       10008       0.70      0.72      0.71        46\n",
      "       10009       0.00      0.00      0.00         4\n",
      "       10010       0.75      0.55      0.63        11\n",
      "       10011       0.00      0.00      0.00         7\n",
      "       10012       0.00      0.00      0.00         4\n",
      "       10013       0.83      0.41      0.55        37\n",
      "       10014       1.00      0.33      0.50         6\n",
      "       10015       0.62      0.62      0.62        24\n",
      "       10016       0.50      0.14      0.22        14\n",
      "       10017       0.00      0.00      0.00        12\n",
      "       10018       0.60      0.30      0.40        10\n",
      "       10019       0.50      0.07      0.12        15\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       500\n",
      "   macro avg       0.45      0.29      0.32       500\n",
      "weighted avg       0.55      0.46      0.45       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_new.predict(text_data[1500:])\n",
    "print(classification_report(Y[1500:], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.71      0.45      0.55       188\n",
      "       10001       0.66      0.28      0.39       104\n",
      "       10002       0.60      0.73      0.66        99\n",
      "       10003       0.32      0.84      0.47       271\n",
      "       10004       1.00      0.53      0.70        15\n",
      "       10005       0.62      0.56      0.59       142\n",
      "       10006       0.74      0.22      0.34       145\n",
      "       10007       1.00      0.80      0.89         5\n",
      "       10008       0.76      0.66      0.71       117\n",
      "       10009       0.60      0.25      0.35        12\n",
      "       10010       0.80      0.27      0.40        45\n",
      "       10011       1.00      0.50      0.67         6\n",
      "       10012       0.83      0.48      0.61        21\n",
      "       10013       0.80      0.52      0.63        67\n",
      "       10014       1.00      0.22      0.36        23\n",
      "       10015       0.84      0.89      0.87        95\n",
      "       10016       0.67      0.13      0.22        45\n",
      "       10017       1.00      0.09      0.16        35\n",
      "       10018       0.70      0.57      0.63        28\n",
      "       10019       0.79      0.30      0.43        37\n",
      "\n",
      "   micro avg       0.53      0.53      0.53      1500\n",
      "   macro avg       0.77      0.46      0.53      1500\n",
      "weighted avg       0.66      0.53      0.53      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model_new.predict(text_data[:1500])\n",
    "print(classification_report(Y[:1500], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
