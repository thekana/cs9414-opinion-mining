{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics, tree\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_selection import chi2, SelectKBest\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.tsv', sep='\\t', quoting=csv.QUOTE_NONE, dtype=str, encoding = 'utf-8',\n",
    "                 header=None, names=[\"instance\", \"text\", \"id\", \"sentiment\", \"is_sarcastic\"])\n",
    "#df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Functions for text pre-processing \"\"\"\n",
    "\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \" \", sample)\n",
    "\n",
    "\n",
    "def remove_punctuation(sample):\n",
    "    \"\"\"Remove punctuations from a sample string\"\"\"\n",
    "#     punctuations = r'''$!\"&'()*+,-./:;<=>?[\\]^`{|}~'''\n",
    "#     no_punct = \"\"\n",
    "#     for char in sample:\n",
    "#         if char not in punctuations:\n",
    "#             no_punct = no_punct + char\n",
    "#     return no_punct\n",
    "    return re.sub(r'[^\\w\\s\\&\\#\\@\\$\\%\\_]','',sample)\n",
    "\n",
    "def myTokenizer(sample):\n",
    "    \"\"\"Customized tokenizer\"\"\"\n",
    "    ################################## 1. Remove numbers\n",
    "    ################################## 2. Remove auspoll thingy\n",
    "    ################################## 3. Remove starts with au\n",
    "    new_words = []\n",
    "    words = sample.split(' ')\n",
    "    new_words = [word for word in words if len(word) >= 2 and not word.startswith('#aus') and not word.startswith('au')] #and not bool(re.search(r'\\d',word))]\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords_NLTK(sample):\n",
    "    \"\"\"Remove stopwords using NLTK\"\"\"\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    filteredText = \"\"\n",
    "    for word in words:\n",
    "        if word not in stopWords:\n",
    "            filteredText = filteredText + word + \" \"\n",
    "    return filteredText.rstrip()\n",
    "\n",
    "\n",
    "def porter_stem(sample):\n",
    "    \"\"\"Stemming\"\"\"\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + ps.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "def lemmy(sample):\n",
    "    #nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    lemmed_text = \"\"\n",
    "    for word in words:\n",
    "        lemmed_text = lemmed_text + lemmatizer.lemmatize(word, pos='v') + \" \"\n",
    "    return lemmed_text.rstrip()\n",
    "    \n",
    "def snowball(sample):\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + stemmer.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "# def myPreprocessor(sample):\n",
    "#     \"\"\"Customized preprocessor\"\"\"\n",
    "#     sample = remove_URL(sample)\n",
    "#     sample = sample.lower()\n",
    "#     sample = remove_punctuation(sample)\n",
    "#     sample = remove_stopwords_NLTK(sample)\n",
    "#     sample = porter_stem(sample)\n",
    "#     return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(input_text):\n",
    "    return re.sub(r'@\\w+', '', input_text)\n",
    "\n",
    "def remove_urls(input_text):\n",
    "    return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "\n",
    "def emoji_oneword(input_text):\n",
    "    # By compressing the underscore, the emoji is kept as one word\n",
    "    return input_text.replace('_','')\n",
    "\n",
    "def remove_punctuation(input_text):\n",
    "    # Make translation table\n",
    "    punct = string.punctuation\n",
    "    trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "    return input_text.translate(trantab)\n",
    "\n",
    "def remove_digits(input_text):\n",
    "    return re.sub('\\d+', '', input_text)\n",
    "\n",
    "def to_lower(input_text):\n",
    "    return input_text.lower()\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    # whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list) and len(word) > 1] \n",
    "    return \" \".join(clean_words) \n",
    "\n",
    "def stemming(input_text):\n",
    "    porter = PorterStemmer()\n",
    "    words = input_text.split() \n",
    "    stemmed_words = [porter.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "def newProcess(sample):\n",
    "    sample = remove_mentions(sample)\n",
    "    sample = remove_urls(sample)\n",
    "    sample = emoji_oneword(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "    sample = remove_digits(sample)\n",
    "    sample = to_lower(sample)\n",
    "    sample = remove_stopwords(sample)\n",
    "    sample = stemming(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data creation \"\"\"\n",
    "text_data = np.array([])\n",
    "# Read tweets\n",
    "for text in df.text:\n",
    "    text_data = np.append(text_data, text)\n",
    "# creating target classes\n",
    "Y = np.array([])\n",
    "for text in df.id:\n",
    "    Y = np.append(Y, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_test_, y_train, y_test = train_test_split(text_data, Y, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "700\n"
     ]
    }
   ],
   "source": [
    "# try to use sklearn stop_words later\n",
    "# 711, 0.688\n",
    "# 1178, 0.978\n",
    "# max_features=818, ngram_range=(1, 2), min_df = 0\n",
    "count = CountVectorizer(preprocessor=newProcess, tokenizer=myTokenizer, max_features=700, ngram_range=(1, 1), min_df = 4, max_df = 0.2)\n",
    "X_train = count.fit_transform(X_train_).toarray()\n",
    "X_test = count.transform(X_test_).toarray()\n",
    "# print(count.get_feature_names())\n",
    "# size = len(count.vocabulary_)\n",
    "print(len(count.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha = 0.75)\n",
    "model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.60      0.66      0.63        56\n",
      "       10001       0.38      0.28      0.32        36\n",
      "       10002       0.56      0.45      0.50        31\n",
      "       10003       0.36      0.55      0.43        87\n",
      "       10004       0.00      0.00      0.00         2\n",
      "       10005       0.63      0.69      0.66        52\n",
      "       10006       0.40      0.41      0.40        44\n",
      "       10007       0.00      0.00      0.00         2\n",
      "       10008       0.60      0.74      0.66        46\n",
      "       10009       0.00      0.00      0.00         4\n",
      "       10010       0.22      0.18      0.20        11\n",
      "       10011       0.00      0.00      0.00         7\n",
      "       10012       0.00      0.00      0.00         4\n",
      "       10013       0.77      0.62      0.69        37\n",
      "       10014       0.00      0.00      0.00         6\n",
      "       10015       0.55      0.67      0.60        24\n",
      "       10016       0.25      0.07      0.11        14\n",
      "       10017       0.00      0.00      0.00        12\n",
      "       10018       0.50      0.30      0.37        10\n",
      "       10019       0.38      0.20      0.26        15\n",
      "\n",
      "   micro avg       0.49      0.49      0.49       500\n",
      "   macro avg       0.31      0.29      0.29       500\n",
      "weighted avg       0.46      0.49      0.47       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.84      0.81      0.82       188\n",
      "       10001       0.75      0.67      0.71       104\n",
      "       10002       0.71      0.81      0.75        99\n",
      "       10003       0.69      0.73      0.71       271\n",
      "       10004       1.00      0.60      0.75        15\n",
      "       10005       0.73      0.84      0.78       142\n",
      "       10006       0.83      0.76      0.79       145\n",
      "       10007       0.00      0.00      0.00         5\n",
      "       10008       0.80      0.81      0.81       117\n",
      "       10009       0.73      0.67      0.70        12\n",
      "       10010       0.56      0.71      0.63        45\n",
      "       10011       1.00      0.17      0.29         6\n",
      "       10012       0.76      0.62      0.68        21\n",
      "       10013       0.77      0.87      0.82        67\n",
      "       10014       0.92      0.52      0.67        23\n",
      "       10015       0.81      0.96      0.88        95\n",
      "       10016       0.66      0.60      0.63        45\n",
      "       10017       0.82      0.51      0.63        35\n",
      "       10018       0.73      0.68      0.70        28\n",
      "       10019       0.88      0.57      0.69        37\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      1500\n",
      "   macro avg       0.75      0.65      0.67      1500\n",
      "weighted avg       0.76      0.76      0.75      1500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.756"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_new.predict(text_data[1500:])\n",
    "# print(classification_report(Y[1500:], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.48648649 0.45925926 0.49875312 0.50125945 0.51538462]\n",
      "F1 micro Accuracy: 0.49 (+/- 0.04)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "clf = make_pipeline(CountVectorizer(preprocessor=newProcess, tokenizer=myTokenizer, max_features=700, ngram_range=(1, 1), min_df = 4, max_df = 0.2), MultinomialNB(alpha = 0.75))\n",
    "scores = cross_val_score(clf,text_data,Y,cv=5,scoring = 'f1_micro')\n",
    "print(scores)\n",
    "print(\"F1 micro Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_new.predict(text_data[:1500])\n",
    "# print(classification_report(Y[:1500], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
