{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "import sys\n",
    "import time\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn import metrics, tree\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics import (accuracy_score, classification_report,\n",
    "                             confusion_matrix, f1_score, precision_score,\n",
    "                             recall_score)\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
    "from sklearn.feature_selection import chi2, SelectKBest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.tsv', sep='\\t', quoting=csv.QUOTE_NONE, dtype=str, encoding = 'utf-8',\n",
    "                 header=None, names=[\"instance\", \"text\", \"id\", \"sentiment\", \"is_sarcastic\"])\n",
    "#df = shuffle(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Functions for text pre-processing \"\"\"\n",
    "\n",
    "\n",
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \" \", sample)\n",
    "\n",
    "\n",
    "def remove_punctuation(sample):\n",
    "    \"\"\"Remove punctuations from a sample string\"\"\"\n",
    "#     punctuations = r'''$!\"&'()*+,-./:;<=>?[\\]^`{|}~'''\n",
    "#     no_punct = \"\"\n",
    "#     for char in sample:\n",
    "#         if char not in punctuations:\n",
    "#             no_punct = no_punct + char\n",
    "#     return no_punct\n",
    "    return re.sub(r'[^\\w\\s\\&\\#\\@\\$\\%\\_]','',sample)\n",
    "\n",
    "def myTokenizer(sample):\n",
    "    \"\"\"Customized tokenizer\"\"\"\n",
    "    ################################## 1. Remove numbers\n",
    "    ################################## 2. Remove auspoll thingy\n",
    "    ################################## 3. Remove starts with au\n",
    "    new_words = []\n",
    "    words = sample.split(' ')\n",
    "    new_words = [word for word in words if len(word) >= 2 and not word.isdigit() and not word.startswith('#aus') and not word.startswith('au') and not bool(re.search(r'\\d',word))]\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords_NLTK(sample):\n",
    "    \"\"\"Remove stopwords using NLTK\"\"\"\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    filteredText = \"\"\n",
    "    for word in words:\n",
    "        if word not in stopWords:\n",
    "            filteredText = filteredText + word + \" \"\n",
    "    return filteredText.rstrip()\n",
    "\n",
    "\n",
    "def porter_stem(sample):\n",
    "    \"\"\"Stemming\"\"\"\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + ps.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "def lemmy(sample):\n",
    "    #nltk.download('wordnet')\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    lemmed_text = \"\"\n",
    "    for word in words:\n",
    "        lemmed_text = lemmed_text + lemmatizer.lemmatize(word) + \" \"\n",
    "    return lemmed_text.rstrip()\n",
    "    \n",
    "def snowball(sample):\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + stemmer.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "def myPreprocessor(sample):\n",
    "    \"\"\"Customized preprocessor\"\"\"\n",
    "    sample = remove_URL(sample)\n",
    "    sample = sample.lower()\n",
    "    sample = remove_stopwords_NLTK(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "#     sample = lemmy(sample)\n",
    "#     sample = snowball(sample)\n",
    "    sample = porter_stem(sample)\n",
    "    return sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#myTokenizer('While the majority of all Internet search engines utilize stop words, they do not prevent a user from using them, but they are ignored.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data creation \"\"\"\n",
    "text_data = np.array([])\n",
    "# Read tweets\n",
    "for text in df.text:\n",
    "    text_data = np.append(text_data, text)\n",
    "# creating target classes\n",
    "Y = np.array([])\n",
    "for text in df.id:\n",
    "    Y = np.append(Y, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_test_, y_train, y_test = train_test_split(text_data, Y, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#afpraid', '#alp', '#asylumseek', '#betterfutur', '#brexit', '#cfmeu', '#chafta', '#cleanenergi', '#climatechang', '#climatesci', '#coal', '#corrupt', '#csg', '#csiro', '#csirocut', '#csirocut #climatesci', '#dutton', '#educ', '#et', '#fail', '#faketradi', '#fraudband', '#gonski', '#green', '#humanright', '#icac', '#inequ', '#insid', '#labor', '#labor posit', '#labor posit plan', '#laborlaunch', '#latelin', '#leadersdeb', '#liber', '#lnp', '#lnp doctor', '#lnp doctor &amp', '#lnpfail', '#lnpliar', '#marriageequ', '#medicar', '#nauru', '#nbn', '#nbn plan', '#nbnco', '#nbngate', '#negativegear', '#npc', '#panamapap', '#parakeelia', '#peoplesforum', '#putlnplast', '#qanda', '#qldpol', '#refuge', '#refuge #dutton', '#renew', '#safeschool', '#savemedicar', '#scomo', '#spendomet', '#springst', '#ssm', '#ssm plebiscit', '#stopstateterror', '#stopstateterror #lnp', '#taxrort', '#tennew', '#thedrum', '#turnbul', '$$$', '&amp', '&amp #alp', '&amp #lnp', '&amp #lnp doctor', '&amp estat', '&amp estat agent', '&amp fund', '&amp green', '&amp growth', '&amp lnp', '&amp turnbul', '&gt', '@abcnew', '@albomp', '@australianlabor', '@barnaby_joyc', '@billshortenmp', '@billshortenmp say', '@bowenchri', '@cpyne', '@green', '@greghuntmp', '@juliebishopmp', '@liberalau', '@mathiascormann', '@peterdutton_mp', '@richarddinatal', '@scottmorrisonmp', '@skynewsaust', '@smh', '@tanya_plibersek', '@theag', '@tonyabbottmhr', '@tonyhwindsor', '@turnbullmalcolm', '@turnbullmalcolm say', '@turnbullmalcolm want', 'aaa', 'aaa credit', 'aaa credit rate', 'abbott', 'abc', 'abc sb', 'abl', 'abort', 'absolut', 'abus', 'account', 'action', 'actual', 'ad', 'admit', 'adopt', 'afford', 'afford hous', 'afford hous fair', 'afford housingfair', 'afford housingfair go', 'afford school', 'afp', 'afp raid', 'again', 'age', 'agenda', 'agent', 'agent &amp #lnp', 'aid', 'all', 'allow', 'alon', 'alp', 'alreadi', 'also', 'alway', 'announc', 'anoth', 'answer', 'anyon', 'anyth', 'around', 'ask', 'asylum', 'asylum seeker', 'attack', 'avoid', 'away', 'back', 'ban', 'bank', 'barnabi', 'barnabi joyc', 'becom', 'behind', 'believ', 'benefit', 'better', 'big', 'big busi', 'biggest', 'bill', 'bill shorten', 'bill shorten say', 'billion', 'bishop', 'black', 'black hole', 'blame', 'bloodi', 'boat', 'border', 'bowen', 'break', 'brexit', 'britain', 'broadband', 'broken', 'budget', 'build', 'bulk', 'busi', 'buy', 'call', 'camp', 'campaign', 'candid', 'cannot', 'cant', 'cant afford', 'carbon', 'carbon tax', 'care', 'cattl', 'caus', 'cayman', 'centr', 'ceo', 'cfa', 'chang', 'check', 'child', 'childcar', 'chri', 'claim', 'class', 'clean', 'climat', 'climat chang', 'co', 'coal', 'coalit', 'come', 'comment', 'commiss', 'commit', 'commun', 'compani', 'compani tax', 'compani tax cut', 'concern', 'confid', 'confirm', 'contact', 'continu', 'copper', 'cormann', 'corp', 'corp tax', 'corpor', 'corpor tax', 'corpor tax cut', 'corrupt', 'cost', 'could', 'council', 'countri', 'creat', 'credibl', 'credit', 'crimin', 'criticis', 'csiro', 'cut', 'cut health', 'cut health educ', 'cut penalti', 'cut penalti rate', 'dairi', 'dairi farmer', 'damag', 'day', 'deal', 'debat', 'debt', 'decis', 'declar', 'defend', 'deficit', 'deliv', 'democraci', 'deni', 'deserv', 'desper', 'destroy', 'detail', 'detent', 'develop', 'di', 'di natal', 'differ', 'digit', 'dinner', 'direct', 'direct action', 'disgrac', 'disput', 'distract', 'doctor', 'doctor &amp', 'doctor &amp #alp', 'document', 'dodgi', 'dog', 'dollar', 'donat', 'done', 'donor', 'dont', 'dr', 'drug', 'due', 'dumb', 'dutton', 'dutton illiter', 'earli', 'earner', 'earner childcar', 'earner childcar support', 'econom', 'econom growth', 'econom manag', 'econom plan', 'economi', 'educ', 'educ health', 'effect', 'elect', 'elector', 'electr', 'end', 'energi', 'english', 'ensur', 'equal', 'etc', 'even', 'ever', 'everi', 'everi time', 'everyon', 'everyth', 'evid', 'expect', 'expens', 'expert', 'explain', 'export', 'expos', 'extra', 'fact', 'fail', 'fair', 'fall', 'famili', 'far', 'farmer', 'fault', 'fear', 'feder', 'ff', 'final', 'find', 'first', 'fix', 'fool', 'forc', 'forget', 'former', 'free', 'freez', 'full', 'fund', 'futur', 'gay', 'gdp', 'gear', 'get', 'gfc', 'give', 'given', 'global', 'go', 'gone', 'good', 'got', 'gov', 'govern', 'govt', 'gp', 'great', 'green', 'greg', 'greg hunt', 'grow', 'growth', 'gst', 'guarante', 'half', 'hand', 'happen', 'hard', 'hate', 'haven', 'he', 'head', 'headspac', 'health', 'health educ', 'hear', 'heard', 'help', 'here', 'hey', 'hide', 'high', 'higher', 'hit', 'hockey', 'hole', 'home', 'hope', 'hospit', 'hous', 'human', 'hung', 'hunt', 'hurt', 'id', 'idea', 'if', 'ignor', 'ill', 'illeg', 'illiter', 'illiter refuge', 'im', 'imagin', 'immigr', 'immigr minist', 'impact', 'import', 'in', 'increas', 'independ', 'indonesia', 'industri', 'inform', 'innov', 'instead', 'integr', 'interest', 'intern', 'internet', 'internship', 'invest', 'investig', 'is', 'islam', 'islamist', 'island', 'isnt', 'issu', 'it', 'job', 'job &amp', 'job &amp growth', 'job growth', 'john', 'joke', 'joyc', 'joyc link', 'juli', 'juli bishop', 'keep', 'kelli', 'kid', 'kill', 'knew', 'know', 'labor', 'labor commit', 'labor govern', 'labor parti', 'labor want', 'lack', 'last', 'last time', 'launder', 'law', 'lead', 'leader', 'leader bill', 'leader bill shorten', 'leak', 'learn', 'left', 'legal', 'less', 'let', 'level', 'liar', 'lib', 'liber', 'liber candid', 'liber cut', 'liber last', 'liber parti', 'lie', 'like', 'link', 'listen', 'littl', 'live', 'lnp', 'lnp govt', 'lnp last', 'lnp want', 'loan', 'local', 'look', 'lose', 'lost', 'lot', 'love', 'low', 'lower', 'major', 'make', 'mal', 'malcolm', 'malcolm turnbul', 'manag', 'mani', 'manu', 'market', 'marriag', 'marriag equal', 'marriag plebiscit', 'mate', 'mathia', 'may', 'mean', 'media', 'medicar', 'member', 'mental', 'mention', 'million', 'mine', 'minist', 'money', 'money launder', 'morrison', 'mp', 'mr', 'mr turnbul', 'msm', 'mt', 'much', 'multin', 'muslim', 'must', 'name', 'natal', 'nation', 'nauru', 'nbn', 'need', 'neg', 'neg gear', 'never', 'new', 'news', 'next', 'no', 'noth', 'now', 'nsw', 'off', 'offer', 'offic', 'offshor', 'ok', 'old', 'on', 'one', 'open', 'opposit', 'order', 'out', 'oversea', 'owner', 'oz', 'paid', 'panama', 'panama paper', 'paper', 'parakeelia', 'parent', 'part', 'parti', 'pass', 'past', 'patient', 'pay', 'pay tax', 'payment', 'penalti', 'penalti rate', 'pension', 'peopl', 'person', 'perth', 'peter', 'peter dutton', 'pl', 'plan', 'pleas', 'plebiscit', 'pledg', 'pm', 'pm turnbul', 'point', 'polic', 'polici', 'polit', 'polit parti', 'politician', 'poll', 'poor', 'posit', 'posit plan', 'power', 'ppl', 'price', 'prime', 'privat', 'privatis', 'privatis #medicar', 'privatis medicar', 'problem', 'profit', 'program', 'promis', 'properti', 'protect', 'protest', 'prove', 'provid', 'public', 'put', 'put liber', 'put liber last', 'put lnp', 'put lnp last', 'question', 'racism', 'raid', 'rais', 'rate', 'rba', 'read', 'real', 'realli', 'reason', 'rebat', 'record', 'reduc', 'reef', 'reform', 'refuge', 'refus', 'rememb', 'remov', 'renew', 'renew energi', 'report', 'respons', 'return', 'reveal', 'revers', 'rhetor', 'rich', 'right', 'rise', 'risk', 'roll', 'rort', 'royal', 'royal commiss', 'run', 'sack', 'safe', 'said', 'samesex', 'samesex marriag', 'save', 'say', 'say @richarddinatal', 'say labor', 'sb', 'scandal', 'scare', 'scare campaign', 'school', 'scott', 'scott morrison', 'scrap', 'screw', 'seat', 'second', 'secret', 'sector', 'secur', 'see', 'seek', 'seeker', 'seem', 'sell', 'senat', 'servic', 'set', 'shame', 'shame labor', 'shit', 'shock', 'shonki', 'shonki bank', 'shonki bank &amp', 'shoot', 'shorten', 'shorten open', 'shorten promis', 'shorten say', 'shorten say labor', 'show', 'shred', 'shut', 'sign', 'silent', 'sinc', 'singl', 'sky', 'slash', 'slow', 'small', 'small busi', 'so', 'social', 'someth', 'soon', 'speak', 'spend', 'spent', 'ssm', 'stabil', 'stage', 'stand', 'start', 'state', 'statement', 'still', 'stop', 'stori', 'strategi', 'strong', 'student', 'stupid', 'submarin', 'suicid', 'super', 'superannu', 'support', 'sure', 'surplu', 'surpris', 'system', 'system afford', 'system afford housingfair', 'tafe', 'tafe climat', 'tafe climat chang', 'take', 'take job', 'talk', 'talk health', 'target', 'target low', 'target low middleincom', 'tax', 'tax avoid', 'tax cut', 'tax cut corpor', 'tax haven', 'tax hurt', 'tax rate', 'tax system', 'tax system afford', 'taxat', 'taxat who', 'taxat who knew', 'taxpay', 'taxpay money', 'tell', 'term', 'terror', 'terrorist', 'test', 'thank', 'that', 'the', 'them', 'there', 'theyll', 'theyr', 'thi', 'thing', 'think', 'though', 'thousand', 'threat', 'three', 'throw', 'till', 'time', 'time cut', 'today', 'told', 'toni', 'toni abbott', 'took', 'top', 'total', 'town', 'trade', 'transfer', 'transit', 'transpar', 'treasur', 'treasur @scottmorrisonmp', 'tri', 'trickl', 'true', 'trump', 'trust', 'trustpm', 'trustpm shonki', 'trustpm shonki bank', 'turn', 'turn back', 'turnbul', 'turnbul #panamapap', 'turnbul &amp', 'turnbul absolut', 'turnbul govern', 'turnbul govt', 'turnbul job', 'turnbul name', 'turnbul plan', 'turnbul say', 'turnbul want', 'two', 'uk', 'unemploy', 'union', 'urg', 'us', 'use', 'usual', 'valu', 'via', 'via @abcnew', 'via @smh', 'via @theag', 'victoria', 'visa', 'visit', 'volunt', 'vote', 'vote #lnp', 'vote labor', 'vote lnp', 'voter', 'vulner', 'wa', 'wage', 'wait', 'wake', 'want', 'want cut', 'want know', 'war', 'warn', 'wast', 'water', 'way', 'we', 'wealthi', 'week', 'weird', 'weird peopl', 'weird peopl care', 'welfar', 'well', 'were', 'weve', 'what', 'white', 'who', 'who knew', 'who knew #lnpfail', 'who trustpm', 'who trustpm shonki', 'win', 'within', 'without', 'women', 'wonder', 'wont', 'word', 'work', 'worker', 'world', 'wors', 'worst', 'worth', 'would', 'wrong', 'xenophobia', 'ye', 'yeah', 'year', 'yesterday', 'yet', 'you', 'young', 'your', 'yr', 'zero']\n",
      "930\n"
     ]
    }
   ],
   "source": [
    "# try to use sklearn stop_words later\n",
    "# 711, 0.688\n",
    "# 1178, 0.978\n",
    "count = CountVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer, max_features=930, ngram_range=(1, 3), min_df = 4)\n",
    "# count = TfidfVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer, max_features=800,sublinear_tf=True, min_df=5,\n",
    "#                         ngram_range=(1, 2), \n",
    "#                         stop_words='english')\n",
    "# bag_of_words = count.fit_transform(text_data)\n",
    "X_train = count.fit_transform(X_train_).toarray()\n",
    "X_test = count.transform(X_test_).toarray()\n",
    "print(count.get_feature_names())\n",
    "# size = len(count.vocabulary_)\n",
    "print(len(count.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB(alpha = 1.0, fit_prior = True)\n",
    "#clf = BernoulliNB(alpha = 1.0, fit_prior = False)\n",
    "model = clf.fit(X_train, y_train)\n",
    "#model = clf.fit(X, Y)\n",
    "#print(model.class_log_prior_ )\n",
    "# text_clf_red = Pipeline([('vect', CountVectorizer(preprocessor=myPreprocessor, tokenizer=myTokenizer)), \n",
    "#                        ('reducer', SelectKBest(chi2, k=200)),\n",
    "#                        ('clf', MultinomialNB())\n",
    "#                        ])\n",
    "# model_new = text_clf_red.fit(text_data[:1500],Y[:1500])\n",
    "# model_new = text_clf_red.fit(text_data,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.58      0.70      0.63        56\n",
      "       10001       0.35      0.25      0.29        36\n",
      "       10002       0.56      0.58      0.57        31\n",
      "       10003       0.31      0.53      0.39        87\n",
      "       10004       0.00      0.00      0.00         2\n",
      "       10005       0.58      0.62      0.60        52\n",
      "       10006       0.42      0.36      0.39        44\n",
      "       10007       0.00      0.00      0.00         2\n",
      "       10008       0.60      0.67      0.63        46\n",
      "       10009       0.00      0.00      0.00         4\n",
      "       10010       0.33      0.27      0.30        11\n",
      "       10011       0.00      0.00      0.00         7\n",
      "       10012       0.00      0.00      0.00         4\n",
      "       10013       0.57      0.35      0.43        37\n",
      "       10014       0.00      0.00      0.00         6\n",
      "       10015       0.52      0.62      0.57        24\n",
      "       10016       0.25      0.07      0.11        14\n",
      "       10017       0.00      0.00      0.00        12\n",
      "       10018       0.50      0.30      0.37        10\n",
      "       10019       0.50      0.20      0.29        15\n",
      "\n",
      "   micro avg       0.46      0.46      0.46       500\n",
      "   macro avg       0.30      0.28      0.28       500\n",
      "weighted avg       0.44      0.46      0.44       500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doodey\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.81      0.84      0.82       188\n",
      "       10001       0.79      0.65      0.72       104\n",
      "       10002       0.69      0.80      0.74        99\n",
      "       10003       0.69      0.74      0.71       271\n",
      "       10004       1.00      0.53      0.70        15\n",
      "       10005       0.70      0.82      0.76       142\n",
      "       10006       0.76      0.81      0.79       145\n",
      "       10007       0.00      0.00      0.00         5\n",
      "       10008       0.78      0.84      0.81       117\n",
      "       10009       0.78      0.58      0.67        12\n",
      "       10010       0.55      0.64      0.59        45\n",
      "       10011       0.00      0.00      0.00         6\n",
      "       10012       0.73      0.52      0.61        21\n",
      "       10013       0.80      0.84      0.82        67\n",
      "       10014       1.00      0.30      0.47        23\n",
      "       10015       0.80      0.95      0.87        95\n",
      "       10016       0.75      0.47      0.58        45\n",
      "       10017       0.85      0.49      0.62        35\n",
      "       10018       0.73      0.68      0.70        28\n",
      "       10019       0.88      0.57      0.69        37\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      1500\n",
      "   macro avg       0.71      0.60      0.63      1500\n",
      "weighted avg       0.75      0.75      0.74      1500\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Doodey\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_train)\n",
    "print(classification_report(y_train, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_new.predict(text_data[1500:])\n",
    "# print(classification_report(Y[1500:], y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred = model_new.predict(text_data[:1500])\n",
    "# print(classification_report(Y[:1500], y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
