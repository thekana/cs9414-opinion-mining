{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv, sys, re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_csv('dataset.tsv', sep='\\t', quoting=csv.QUOTE_NONE , dtype= str, header = None, names=[\"instance\", \"text\", \"id\", \"sentiment\",\"is_sarcastic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance</th>\n",
       "      <th>text</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>is_sarcastic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>71</td>\n",
       "      <td>When Turnbull babbles on about science &amp;amp; i...</td>\n",
       "      <td>10012</td>\n",
       "      <td>negative</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1951</th>\n",
       "      <td>1952</td>\n",
       "      <td>? VOTE LNP OUT ? TURNBULL IS TRYING TO KEEP NB...</td>\n",
       "      <td>10015</td>\n",
       "      <td>negative</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1515</th>\n",
       "      <td>1516</td>\n",
       "      <td>First he shut down @abcfactcheck. Now @Turnbul...</td>\n",
       "      <td>10015</td>\n",
       "      <td>negative</td>\n",
       "      <td>false</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     instance                                               text     id  \\\n",
       "70         71  When Turnbull babbles on about science &amp; i...  10012   \n",
       "1951     1952  ? VOTE LNP OUT ? TURNBULL IS TRYING TO KEEP NB...  10015   \n",
       "1515     1516  First he shut down @abcfactcheck. Now @Turnbul...  10015   \n",
       "\n",
       "     sentiment is_sarcastic  \n",
       "70    negative        false  \n",
       "1951  negative        false  \n",
       "1515  negative        false  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=shuffle(df)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df.text:\n",
    "    text_data = np.append(text_data,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "\n",
    "def remove_punctuation(sample):\n",
    "    \"\"\"Remove punctuations from a sample string\"\"\"\n",
    "    punctuations = '''!\"&'()*+,-./:;<=>?[\\]^`{|}~'''\n",
    "    no_punct = \"\"\n",
    "    for char in sample:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    return no_punct\n",
    "\n",
    "\n",
    "def myPreprocessor(sample):\n",
    "    \"\"\"Customized preprocessor\"\"\"\n",
    "    sample = remove_URL(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "    return sample\n",
    "\n",
    "\n",
    "def myTokenizer(sample):\n",
    "    \"\"\"Customized tokenizer\"\"\"\n",
    "    new_words = []\n",
    "    words = sample.split(' ')\n",
    "    new_words = [word for word in words if len(word) >= 2]\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#AFPRaids', '#AusPol', '#AusVotes', '#Auspol', '#Ausvotes', '#Brexit', '#Election2016', '#LNP', '#LNPfail', '#Labor', '#Medicare', '#NBN', '#Parakeelia', '#auspol', '#ausvotes', '#ausvotes2016', '#insiders', '#npc', '#qanda', '@AustralianLabor', '@LiberalAus', '@RichardDiNatale', '@TurnbullMalcolm', '@billshortenmp', 'AFP', 'ALP', 'Abbott', 'Australia', 'Australian', 'Australians', 'Bill', 'Coalition', 'Dutton', 'Government', 'Govt', 'Greens', 'How', 'If', 'Its', 'Joyce', 'LNP', 'Labor', 'Labors', 'Liberal', 'Liberals', 'Libs', 'Malcolm', 'Medicare', 'Morrison', 'NBN', 'No', 'Not', 'PM', 'Party', 'Peter', 'Shorten', 'So', 'THE', 'The', 'This', 'Turnbull', 'Turnbulls', 'VOTE', 'Vote', 'We', 'What', 'Why', 'You', 'about', 'after', 'again', 'all', 'amp', 'an', 'and', 'any', 'are', 'as', 'at', 'back', 'be', 'because', 'been', 'big', 'boats', 'budget', 'business', 'but', 'by', 'campaign', 'can', 'cant', 'care', 'could', 'cut', 'cuts', 'did', 'do', 'dont', 'down', 'economic', 'economy', 'education', 'election', 'even', 'for', 'from', 'funding', 'gearing', 'get', 'going', 'good', 'government', 'govt', 'growth', 'had', 'has', 'have', 'he', 'health', 'his', 'how', 'if', 'in', 'into', 'is', 'it', 'its', 'jobs', 'just', 'know', 'labor', 'last', 'like', 'money', 'more', 'must', 'need', 'no', 'not', 'now', 'of', 'on', 'one', 'only', 'or', 'our', 'out', 'over', 'own', 'party', 'pay', 'people', 'plan', 'plans', 'plebiscite', 'policy', 'put', 'refugees', 'right', 'say', 'says', 'schools', 'so', 'support', 'take', 'tax', 'taxes', 'than', 'that', 'the', 'their', 'them', 'there', 'they', 'think', 'this', 'time', 'to', 'under', 'up', 'us', 'via', 'vote', 'want', 'wants', 'was', 'we', 'what', 'when', 'who', 'why', 'will', 'with', 'wont', 'work', 'workers', 'would', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(preprocessor=myPreprocessor, lowercase = True, tokenizer=myTokenizer ,max_features = 200)\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "print(count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Turnbull': 60, 'on': 142, 'about': 68, 'amp': 72, 'want': 184, '#LNP': 7, 'has': 116, 'to': 178, 'the': 170, 'down': 99, '#auspol': 13, '#ausvotes': 14, 'VOTE': 62, 'LNP': 40, 'NBN': 49, 'THE': 57, 'he': 118, '@TurnbullMalcolm': 22, 'ALP': 25, 'of': 141, 'his': 120, '#NBN': 11, 'Shorten': 55, 'no': 138, 'for': 105, 'PM': 52, 'Turnbulls': 61, 'are': 76, 'in': 123, 'right': 159, 'our': 146, 'AFP': 24, 'Labor': 41, 'was': 186, 'Peter': 54, 'Dutton': 32, 'refugees': 158, 'jobs': 128, 'what': 188, 'own': 149, 'says': 161, '@billshortenmp': 23, 'Medicare': 47, 'that': 169, 'people': 152, 'care': 92, 'education': 102, 'health': 119, 'who': 190, '#LNPfail': 8, 'now': 140, 'business': 86, 'be': 80, 'and': 74, 'If': 37, 'Australians': 29, 'into': 124, 'how': 121, '@LiberalAus': 20, 'you': 198, 'they': 174, 'at': 78, 'not': 139, 'good': 111, 'The': 58, 'Libs': 45, 'cut': 94, 'them': 172, 'will': 192, '#insiders': 16, '#Parakeelia': 12, 'this': 176, 'cuts': 95, 'Morrison': 48, 'an': 73, 'have': 117, 'is': 125, 'Its': 38, 'going': 110, 'Malcolm': 46, 'like': 133, 'vote': 183, '#ausvotes2016': 15, 'there': 173, 'if': 122, 'do': 97, 'it': 126, 'so': 163, 'Abbott': 26, 'we': 187, '#Brexit': 5, 'but': 87, '#Medicare': 10, '#Labor': 9, 'its': 127, 'Government': 33, 'work': 195, 'did': 96, 'last': 132, 'time': 177, 'Greens': 35, 'policy': 156, 'more': 135, 'by': 88, 'via': 182, 'schools': 162, 'tax': 166, '#npc': 17, 'Australia': 27, 'up': 180, 'Labors': 42, 'their': 171, 'just': 129, 'cant': 91, 'even': 104, 'one': 143, 'plebiscite': 155, 'How': 36, 'What': 65, 'Liberal': 43, 'plan': 153, '#qanda': 18, 'or': 145, 'with': 193, 'as': 77, 'all': 71, 'from': 106, 'growth': 114, 'get': 109, 'No': 50, 'Australian': 28, 'support': 164, '@AustralianLabor': 19, '#Ausvotes': 4, 'over': 148, 'say': 160, 'economic': 100, 'Party': 53, 'only': 144, 'can': 90, 'any': 75, 'party': 150, 'wants': 185, 'would': 197, 'take': 165, 'funding': 107, 'election': 103, 'gearing': 108, 'again': 70, 'your': 199, 'Vote': 63, 'So': 56, 'than': 168, 'us': 181, 'labor': 131, 'wont': 194, 'Liberals': 44, 'know': 130, '#AusVotes': 2, '#AusPol': 1, 'must': 136, 'campaign': 89, 'You': 67, 'had': 115, 'money': 134, 'put': 157, 'need': 137, 'budget': 85, 'back': 79, 'out': 147, 'Bill': 30, 'This': 59, 'government': 112, 'workers': 196, '#Auspol': 3, 'Joyce': 39, 'plans': 154, 'Coalition': 31, 'pay': 151, 'big': 83, 'under': 179, 'taxes': 167, '#Election2016': 6, 'Govt': 34, 'think': 175, 'when': 189, 'because': 81, 'dont': 98, 'We': 64, 'economy': 101, '#AFPRaids': 0, 'could': 93, 'Why': 66, '@RichardDiNatale': 21, 'been': 82, 'boats': 84, 'govt': 113, 'Not': 51, 'after': 69, 'why': 191}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target classes\n",
    "Y = np.array([])\n",
    "for text in df.id:\n",
    "    Y = np.append(Y,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:1500]\n",
    "X_test = X[1500:]\n",
    "y_train = Y[:1500]\n",
    "y_test = Y[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = MultinomialNB()\n",
    "#clf = BernoulliNB()\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy',random_state=0) \n",
    "model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10001' '10017' '10010' '10018' '10000' '10005' '10001' '10018' '10001'\n",
      " '10005' '10003' '10017' '10003' '10001' '10002' '10006' '10005' '10000'\n",
      " '10019' '10000' '10009' '10000' '10003' '10006' '10003' '10008' '10002'\n",
      " '10003' '10008' '10006' '10001' '10003' '10005' '10008' '10001' '10012'\n",
      " '10000' '10001' '10001' '10005' '10000' '10000' '10002' '10007' '10006'\n",
      " '10003' '10000' '10015' '10008' '10001' '10008' '10001' '10000' '10000'\n",
      " '10013' '10005' '10006' '10005' '10003' '10006' '10003' '10001' '10003'\n",
      " '10003' '10005' '10016' '10017' '10005' '10000' '10006' '10002' '10018'\n",
      " '10000' '10009' '10000' '10006' '10000' '10013' '10003' '10003' '10001'\n",
      " '10008' '10002' '10015' '10005' '10006' '10000' '10003' '10006' '10003'\n",
      " '10000' '10003' '10013' '10002' '10016' '10005' '10000' '10005' '10002'\n",
      " '10008' '10003' '10015' '10013' '10013' '10000' '10000' '10003' '10003'\n",
      " '10018' '10003' '10000' '10012' '10003' '10000' '10000' '10002' '10010'\n",
      " '10013' '10003' '10015' '10003' '10003' '10000' '10003' '10018' '10003'\n",
      " '10008' '10005' '10015' '10003' '10006' '10000' '10003' '10012' '10003'\n",
      " '10006' '10006' '10019' '10015' '10013' '10000' '10005' '10006' '10002'\n",
      " '10003' '10000' '10000' '10013' '10000' '10006' '10018' '10016' '10005'\n",
      " '10016' '10008' '10000' '10005' '10001' '10000' '10003' '10001' '10017'\n",
      " '10019' '10015' '10001' '10006' '10003' '10001' '10000' '10008' '10006'\n",
      " '10000' '10002' '10001' '10003' '10001' '10017' '10008' '10008' '10011'\n",
      " '10008' '10008' '10008' '10010' '10005' '10004' '10006' '10015' '10015'\n",
      " '10006' '10015' '10005' '10003' '10019' '10003' '10002' '10006' '10008'\n",
      " '10006' '10001' '10003' '10010' '10008' '10013' '10003' '10015' '10003'\n",
      " '10016' '10008' '10005' '10016' '10001' '10000' '10000' '10005' '10005'\n",
      " '10018' '10019' '10010' '10016' '10013' '10006' '10002' '10012' '10014'\n",
      " '10016' '10002' '10001' '10003' '10000' '10006' '10003' '10006' '10006'\n",
      " '10008' '10003' '10002' '10016' '10013' '10005' '10016' '10005' '10000'\n",
      " '10005' '10006' '10000' '10001' '10005' '10018' '10000' '10005' '10008'\n",
      " '10006' '10008' '10003' '10003' '10008' '10014' '10016' '10000' '10008'\n",
      " '10013' '10015' '10003' '10019' '10004' '10001' '10003' '10006' '10005'\n",
      " '10010' '10003' '10002' '10008' '10005' '10008' '10005' '10008' '10016'\n",
      " '10014' '10002' '10003' '10006' '10005' '10010' '10003' '10015' '10015'\n",
      " '10012' '10002' '10003' '10002' '10001' '10008' '10006' '10002' '10005'\n",
      " '10012' '10005' '10001' '10006' '10012' '10011' '10013' '10003' '10014'\n",
      " '10002' '10015' '10008' '10018' '10001' '10013' '10008' '10017' '10006'\n",
      " '10002' '10000' '10005' '10015' '10001' '10002' '10006' '10003' '10003'\n",
      " '10003' '10003' '10015' '10003' '10003' '10002' '10013' '10008' '10003'\n",
      " '10003' '10006' '10000' '10006' '10000' '10003' '10008' '10001' '10003'\n",
      " '10006' '10016' '10003' '10000' '10001' '10013' '10001' '10005' '10000'\n",
      " '10001' '10013' '10001' '10002' '10003' '10014' '10003' '10003' '10013'\n",
      " '10005' '10006' '10016' '10015' '10012' '10003' '10010' '10015' '10013'\n",
      " '10003' '10014' '10005' '10006' '10003' '10003' '10006' '10002' '10016'\n",
      " '10005' '10001' '10005' '10003' '10002' '10003' '10000' '10000' '10000'\n",
      " '10003' '10015' '10003' '10005' '10011' '10002' '10000' '10003' '10003'\n",
      " '10001' '10011' '10003' '10006' '10006' '10008' '10003' '10001' '10005'\n",
      " '10000' '10006' '10006' '10000' '10005' '10005' '10015' '10003' '10000'\n",
      " '10003' '10003' '10015' '10001' '10000' '10003' '10015' '10005' '10014'\n",
      " '10001' '10003' '10019' '10003' '10003' '10003' '10000' '10003' '10003'\n",
      " '10003' '10013' '10005' '10002' '10000' '10015' '10019' '10003' '10008'\n",
      " '10003' '10008' '10005' '10002' '10003' '10005' '10013' '10003' '10013'\n",
      " '10003' '10006' '10000' '10003' '10019' '10011' '10000' '10008' '10005'\n",
      " '10006' '10002' '10003' '10003' '10007' '10001' '10000' '10002' '10005'\n",
      " '10013' '10003' '10000' '10010' '10006' '10005' '10006' '10000' '10003'\n",
      " '10000' '10008' '10008' '10006' '10000' '10000' '10000' '10003' '10005'\n",
      " '10000' '10006' '10018' '10005' '10000' '10008' '10019' '10008' '10003'\n",
      " '10006' '10000' '10015' '10000' '10013'] ['10000' '10001' '10005' '10013' '10008' '10003' '10015' '10003' '10006'\n",
      " '10006' '10002' '10006' '10001' '10014' '10002' '10003' '10005' '10015'\n",
      " '10013' '10008' '10008' '10006' '10003' '10000' '10005' '10008' '10003'\n",
      " '10001' '10018' '10005' '10001' '10001' '10005' '10013' '10002' '10015'\n",
      " '10000' '10010' '10000' '10005' '10008' '10000' '10003' '10006' '10001'\n",
      " '10003' '10000' '10015' '10005' '10003' '10006' '10008' '10000' '10015'\n",
      " '10019' '10005' '10002' '10006' '10008' '10003' '10000' '10005' '10017'\n",
      " '10000' '10002' '10006' '10003' '10003' '10000' '10001' '10005' '10018'\n",
      " '10000' '10003' '10015' '10012' '10003' '10003' '10013' '10016' '10015'\n",
      " '10003' '10005' '10015' '10003' '10006' '10000' '10015' '10006' '10000'\n",
      " '10000' '10000' '10019' '10005' '10017' '10006' '10015' '10017' '10002'\n",
      " '10001' '10019' '10015' '10006' '10013' '10000' '10000' '10006' '10014'\n",
      " '10006' '10006' '10000' '10013' '10001' '10000' '10006' '10002' '10005'\n",
      " '10015' '10013' '10015' '10003' '10015' '10003' '10015' '10013' '10011'\n",
      " '10015' '10015' '10015' '10003' '10004' '10005' '10005' '10019' '10010'\n",
      " '10003' '10000' '10001' '10000' '10008' '10002' '10003' '10006' '10001'\n",
      " '10008' '10000' '10000' '10017' '10010' '10016' '10010' '10012' '10005'\n",
      " '10008' '10000' '10000' '10003' '10001' '10006' '10008' '10005' '10008'\n",
      " '10012' '10015' '10003' '10006' '10003' '10013' '10000' '10005' '10017'\n",
      " '10002' '10003' '10003' '10003' '10019' '10003' '10008' '10017' '10000'\n",
      " '10008' '10015' '10017' '10002' '10002' '10000' '10016' '10015' '10011'\n",
      " '10015' '10000' '10003' '10019' '10003' '10013' '10012' '10019' '10003'\n",
      " '10000' '10006' '10017' '10008' '10008' '10002' '10003' '10013' '10002'\n",
      " '10000' '10000' '10001' '10013' '10003' '10000' '10002' '10010' '10006'\n",
      " '10008' '10000' '10003' '10006' '10012' '10003' '10003' '10003' '10003'\n",
      " '10015' '10006' '10012' '10000' '10001' '10006' '10001' '10006' '10009'\n",
      " '10000' '10010' '10006' '10000' '10014' '10006' '10015' '10005' '10000'\n",
      " '10018' '10000' '10000' '10005' '10001' '10000' '10008' '10003' '10003'\n",
      " '10001' '10006' '10003' '10002' '10008' '10016' '10005' '10013' '10000'\n",
      " '10012' '10015' '10006' '10013' '10018' '10005' '10009' '10016' '10002'\n",
      " '10003' '10008' '10006' '10013' '10005' '10000' '10000' '10005' '10003'\n",
      " '10003' '10003' '10008' '10003' '10005' '10003' '10010' '10003' '10003'\n",
      " '10003' '10013' '10003' '10002' '10016' '10005' '10001' '10006' '10002'\n",
      " '10015' '10005' '10000' '10014' '10008' '10014' '10003' '10018' '10001'\n",
      " '10003' '10015' '10008' '10017' '10003' '10008' '10010' '10008' '10000'\n",
      " '10002' '10000' '10005' '10015' '10005' '10000' '10006' '10000' '10000'\n",
      " '10008' '10001' '10015' '10001' '10010' '10005' '10019' '10000' '10009'\n",
      " '10018' '10010' '10016' '10005' '10014' '10000' '10006' '10003' '10006'\n",
      " '10000' '10002' '10005' '10000' '10000' '10013' '10005' '10005' '10000'\n",
      " '10005' '10006' '10000' '10002' '10001' '10003' '10003' '10000' '10001'\n",
      " '10005' '10000' '10008' '10003' '10003' '10005' '10002' '10015' '10002'\n",
      " '10014' '10014' '10001' '10003' '10009' '10003' '10003' '10004' '10013'\n",
      " '10010' '10013' '10003' '10008' '10002' '10003' '10000' '10008' '10003'\n",
      " '10018' '10015' '10002' '10016' '10003' '10003' '10000' '10006' '10000'\n",
      " '10003' '10005' '10001' '10002' '10006' '10003' '10008' '10003' '10002'\n",
      " '10000' '10003' '10005' '10006' '10005' '10005' '10015' '10003' '10003'\n",
      " '10000' '10010' '10015' '10000' '10000' '10006' '10015' '10003' '10016'\n",
      " '10003' '10004' '10019' '10018' '10017' '10018' '10015' '10001' '10005'\n",
      " '10000' '10001' '10006' '10001' '10000' '10006' '10013' '10003' '10003'\n",
      " '10005' '10008' '10006' '10002' '10001' '10003' '10015' '10014' '10002'\n",
      " '10012' '10000' '10003' '10018' '10008' '10010' '10000' '10003' '10002'\n",
      " '10006' '10003' '10005' '10012' '10001' '10006' '10006' '10005' '10006'\n",
      " '10001' '10017' '10000' '10003' '10006' '10005' '10005' '10000' '10005'\n",
      " '10000' '10000' '10018' '10006' '10000' '10006' '10013' '10003' '10000'\n",
      " '10008' '10006' '10003' '10005' '10000' '10003' '10005' '10007' '10001'\n",
      " '10013' '10000' '10012' '10019' '10000']\n",
      "[[1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n",
      "0.23\n",
      "0.23\n",
      "0.23\n",
      "0.23\n",
      "0.13156478752764436\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.43      0.51      0.47        67\n",
      "       10001       0.06      0.05      0.06        38\n",
      "       10002       0.27      0.26      0.26        31\n",
      "       10003       0.18      0.15      0.16        99\n",
      "       10004       0.00      0.00      0.00         2\n",
      "       10005       0.32      0.31      0.32        51\n",
      "       10006       0.24      0.24      0.24        50\n",
      "       10007       0.00      0.00      0.00         2\n",
      "       10008       0.21      0.18      0.19        39\n",
      "       10009       0.00      0.00      0.00         2\n",
      "       10010       0.00      0.00      0.00         9\n",
      "       10011       0.00      0.00      0.00         5\n",
      "       10012       0.00      0.00      0.00         8\n",
      "       10013       0.09      0.08      0.09        24\n",
      "       10014       0.11      0.14      0.12         7\n",
      "       10015       0.44      0.64      0.52        25\n",
      "       10016       0.00      0.00      0.00        15\n",
      "       10017       0.00      0.00      0.00         6\n",
      "       10018       0.09      0.10      0.10        10\n",
      "       10019       0.10      0.10      0.10        10\n",
      "\n",
      "   micro avg       0.23      0.23      0.23       500\n",
      "   macro avg       0.13      0.14      0.13       500\n",
      "weighted avg       0.22      0.23      0.22       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y_test, predicted_y)\n",
    "print(model.predict_proba(X_test))\n",
    "print(accuracy_score(y_test, predicted_y))\n",
    "print(precision_score(y_test, predicted_y, average='micro'))\n",
    "print(recall_score(y_test, predicted_y, average='micro'))\n",
    "print(f1_score(y_test, predicted_y, average='micro'))\n",
    "print(f1_score(y_test, predicted_y, average='macro'))\n",
    "print(classification_report(y_test, predicted_y,output_dict= False))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
