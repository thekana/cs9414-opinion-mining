{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import csv, sys, re\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn import tree, metrics\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_csv('dataset.tsv', sep='\\t', quoting=csv.QUOTE_NONE , dtype= str, header = None, names=[\"instance\", \"text\", \"id\", \"sentiment\",\"is_sarcastic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10003    358\n",
       "10000    244\n",
       "10005    194\n",
       "10006    189\n",
       "10008    163\n",
       "10001    140\n",
       "10002    130\n",
       "10015    119\n",
       "10013    104\n",
       "10016     59\n",
       "10010     56\n",
       "10019     52\n",
       "10017     47\n",
       "10018     38\n",
       "10014     29\n",
       "10012     25\n",
       "10004     17\n",
       "10009     16\n",
       "10011     13\n",
       "10007      7\n",
       "Name: id, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=shuffle(df)\n",
    "df.head(3)\n",
    "df.id.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_data = np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in df.text:\n",
    "    text_data = np.append(text_data,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \"\", sample)\n",
    "\n",
    "\n",
    "def remove_punctuation(sample):\n",
    "    \"\"\"Remove punctuations from a sample string\"\"\"\n",
    "    punctuations = '''!\"&'()*+,-./:;<=>?[\\]^`{|}~'''\n",
    "    no_punct = \"\"\n",
    "    for char in sample:\n",
    "        if char not in punctuations:\n",
    "            no_punct = no_punct + char\n",
    "    return no_punct\n",
    "\n",
    "\n",
    "def myPreprocessor(sample):\n",
    "    \"\"\"Customized preprocessor\"\"\"\n",
    "    sample = remove_URL(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "    return sample\n",
    "\n",
    "\n",
    "def myTokenizer(sample):\n",
    "    \"\"\"Customized tokenizer\"\"\"\n",
    "    new_words = []\n",
    "    words = sample.split(' ')\n",
    "    new_words = [word for word in words if len(word) >= 2]\n",
    "    return new_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['#AFPRaids', '#AusPol', '#AusVotes', '#Auspol', '#Ausvotes', '#Brexit', '#Election2016', '#LNP', '#LNPfail', '#Labor', '#Medicare', '#NBN', '#Parakeelia', '#auspol', '#ausvotes', '#ausvotes2016', '#insiders', '#npc', '#qanda', '@AustralianLabor', '@LiberalAus', '@RichardDiNatale', '@TurnbullMalcolm', '@billshortenmp', 'AFP', 'ALP', 'Abbott', 'Australia', 'Australian', 'Australians', 'Bill', 'Coalition', 'Dutton', 'Government', 'Govt', 'Greens', 'How', 'If', 'Its', 'Joyce', 'LNP', 'Labor', 'Labors', 'Liberal', 'Liberals', 'Libs', 'Malcolm', 'Medicare', 'Morrison', 'NBN', 'No', 'Not', 'PM', 'Party', 'Peter', 'Shorten', 'So', 'THE', 'The', 'This', 'Turnbull', 'Turnbulls', 'VOTE', 'Vote', 'We', 'What', 'Why', 'You', 'about', 'after', 'again', 'all', 'amp', 'an', 'and', 'any', 'are', 'as', 'at', 'back', 'be', 'because', 'been', 'big', 'boats', 'budget', 'business', 'but', 'by', 'campaign', 'can', 'cant', 'care', 'could', 'cut', 'cuts', 'did', 'do', 'dont', 'down', 'economic', 'economy', 'education', 'election', 'even', 'for', 'from', 'funding', 'gearing', 'get', 'going', 'good', 'government', 'govt', 'growth', 'had', 'has', 'have', 'he', 'health', 'his', 'how', 'if', 'in', 'into', 'is', 'it', 'its', 'jobs', 'just', 'know', 'labor', 'last', 'like', 'money', 'more', 'must', 'need', 'no', 'not', 'now', 'of', 'on', 'one', 'only', 'or', 'our', 'out', 'over', 'own', 'party', 'pay', 'people', 'plan', 'plans', 'plebiscite', 'policy', 'put', 'refugees', 'right', 'say', 'says', 'schools', 'so', 'support', 'take', 'tax', 'taxes', 'than', 'that', 'the', 'their', 'them', 'there', 'they', 'think', 'this', 'time', 'to', 'under', 'up', 'us', 'via', 'vote', 'want', 'wants', 'was', 'we', 'what', 'when', 'who', 'why', 'will', 'with', 'wont', 'work', 'workers', 'would', 'you', 'your']\n"
     ]
    }
   ],
   "source": [
    "count = CountVectorizer(preprocessor=myPreprocessor, lowercase = True, tokenizer=myTokenizer ,max_features = 200)\n",
    "bag_of_words = count.fit_transform(text_data)\n",
    "print(count.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'If': 37, 'who': 190, 'his': 120, 'from': 106, 'Bill': 30, 'Shorten': 55, 'amp': 72, 'its': 127, 'with': 193, 'own': 149, 'wants': 185, 'to': 178, 'it': 126, '#auspol': 13, '@AustralianLabor': 19, 'have': 117, 'an': 73, 'about': 68, 'budget': 85, '#ausvotes': 14, '#Election2016': 6, 'again': 70, 'up': 180, 'on': 142, 'and': 74, 'pay': 151, 'The': 58, 'will': 192, 'be': 80, 'Australians': 29, '@LiberalAus': 20, 'no': 138, 'funding': 107, 'the': 170, 'Medicare': 47, 'take': 165, 'over': 148, 'by': 88, 'LNP': 40, 'Liberals': 44, 'not': 139, '#Parakeelia': 12, 'are': 76, 'in': 123, 'you': 198, 'need': 137, 'know': 130, 'just': 129, 'your': 199, 'Its': 38, 'of': 141, '#LNP': 7, 'tax': 166, 'cuts': 95, 'what': 188, 'this': 176, 'says': 161, 'is': 125, 'plan': 153, 'would': 197, '#NBN': 11, 'back': 79, 'Not': 51, 'for': 105, 'if': 122, '@TurnbullMalcolm': 22, '@billshortenmp': 23, 'good': 111, 'than': 168, 'that': 169, 'Joyce': 39, 'now': 140, 'We': 64, 'was': 186, 'under': 179, 'Labor': 41, 'AFP': 24, 'Liberal': 43, 'party': 150, 'so': 163, 'ALP': 25, 'NBN': 49, 'via': 182, 'has': 116, 'can': 90, 'cut': 94, 'get': 109, 'No': 50, 'as': 77, 'Peter': 54, 'Dutton': 32, 'want': 184, 'work': 195, 'must': 136, 'they': 174, 'health': 119, 'education': 102, 'their': 171, 'policy': 156, 'Turnbull': 60, 'growth': 114, 'Australia': 27, 'at': 78, 'one': 143, 'Labors': 42, 'Malcolm': 46, '#AFPRaids': 0, 'going': 110, 'election': 103, 'government': 112, 'had': 115, 'plans': 154, 'there': 173, 'do': 97, '#npc': 17, 'Abbott': 26, 'right': 159, 'but': 87, 'think': 175, 'gearing': 108, 'economy': 101, 'when': 189, 'cant': 91, '#ausvotes2016': 15, 'refugees': 158, 'dont': 98, 'like': 133, 'or': 145, 'What': 65, 'people': 152, 'put': 157, 'last': 132, 'So': 56, 'Libs': 45, 'wont': 194, 'support': 164, 'our': 146, 'big': 83, 'them': 172, '#Auspol': 3, 'time': 177, 'How': 36, 'vote': 183, 'jobs': 128, 'taxes': 167, 'more': 135, 'say': 160, 'Why': 66, 'economic': 100, 'all': 71, 'any': 75, 'VOTE': 62, 'Greens': 35, '#Ausvotes': 4, 'only': 144, 'could': 93, 'why': 191, 'after': 69, 'Coalition': 31, 'boats': 84, 'because': 81, 'Morrison': 48, 'he': 118, 'labor': 131, 'out': 147, 'plebiscite': 155, 'THE': 57, '#Medicare': 10, '#insiders': 16, 'us': 181, 'care': 92, 'been': 82, 'Party': 53, 'did': 96, 'govt': 113, '#Labor': 9, 'how': 121, 'Govt': 34, 'campaign': 89, 'Government': 33, 'Vote': 63, 'even': 104, 'we': 187, 'down': 99, 'schools': 162, 'This': 59, 'into': 124, 'Australian': 28, 'money': 134, 'PM': 52, 'Turnbulls': 61, '@RichardDiNatale': 21, '#LNPfail': 8, 'business': 86, '#AusVotes': 2, '#AusPol': 1, '#qanda': 18, '#Brexit': 5, 'You': 67, 'workers': 196}\n"
     ]
    }
   ],
   "source": [
    "print(count.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = bag_of_words.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target classes\n",
    "Y = np.array([])\n",
    "for text in df.id:\n",
    "    Y = np.append(Y,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X[:1500]\n",
    "X_test = X[1500:]\n",
    "y_train = Y[:1500]\n",
    "y_test = Y[1500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clf = MultinomialNB()\n",
    "#clf = BernoulliNB()\n",
    "clf = tree.DecisionTreeClassifier(criterion='entropy',random_state=0) \n",
    "model = clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_y = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.27\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, predicted_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['10003' '10003' '10006' '10000' '10013' '10002' '10008' '10002' '10006'\n",
      " '10003' '10000' '10000' '10006' '10001' '10003' '10003' '10013' '10003'\n",
      " '10008' '10013' '10001' '10006' '10003' '10005' '10006' '10013' '10003'\n",
      " '10006' '10006' '10003' '10000' '10014' '10003' '10002' '10008' '10002'\n",
      " '10003' '10018' '10009' '10008' '10009' '10015' '10006' '10013' '10001'\n",
      " '10015' '10000' '10017' '10001' '10008' '10014' '10015' '10005' '10012'\n",
      " '10015' '10010' '10002' '10005' '10003' '10008' '10016' '10003' '10003'\n",
      " '10019' '10015' '10005' '10005' '10006' '10015' '10001' '10006' '10003'\n",
      " '10003' '10003' '10005' '10018' '10004' '10017' '10008' '10004' '10003'\n",
      " '10006' '10003' '10015' '10006' '10003' '10000' '10017' '10001' '10010'\n",
      " '10003' '10005' '10002' '10003' '10006' '10000' '10012' '10015' '10016'\n",
      " '10003' '10003' '10005' '10017' '10007' '10000' '10016' '10001' '10003'\n",
      " '10006' '10019' '10003' '10008' '10006' '10003' '10006' '10009' '10015'\n",
      " '10016' '10003' '10015' '10000' '10018' '10016' '10002' '10006' '10000'\n",
      " '10000' '10018' '10006' '10013' '10015' '10002' '10003' '10000' '10000'\n",
      " '10003' '10008' '10008' '10001' '10005' '10017' '10003' '10000' '10003'\n",
      " '10006' '10003' '10001' '10013' '10003' '10015' '10000' '10008' '10002'\n",
      " '10006' '10003' '10003' '10010' '10000' '10013' '10000' '10000' '10015'\n",
      " '10001' '10015' '10005' '10005' '10002' '10006' '10005' '10002' '10008'\n",
      " '10003' '10017' '10017' '10003' '10002' '10005' '10003' '10006' '10003'\n",
      " '10000' '10005' '10003' '10005' '10000' '10000' '10011' '10005' '10001'\n",
      " '10008' '10016' '10015' '10015' '10002' '10003' '10017' '10002' '10019'\n",
      " '10015' '10015' '10005' '10001' '10000' '10010' '10003' '10006' '10012'\n",
      " '10006' '10002' '10019' '10003' '10003' '10003' '10015' '10015' '10006'\n",
      " '10006' '10000' '10015' '10000' '10013' '10003' '10013' '10001' '10002'\n",
      " '10001' '10001' '10001' '10008' '10003' '10000' '10005' '10006' '10017'\n",
      " '10000' '10003' '10012' '10001' '10002' '10013' '10019' '10000' '10003'\n",
      " '10000' '10000' '10019' '10013' '10001' '10003' '10003' '10000' '10003'\n",
      " '10001' '10003' '10001' '10008' '10006' '10008' '10015' '10008' '10014'\n",
      " '10008' '10005' '10000' '10015' '10005' '10005' '10003' '10001' '10003'\n",
      " '10008' '10003' '10018' '10001' '10003' '10015' '10006' '10003' '10008'\n",
      " '10008' '10013' '10018' '10012' '10001' '10016' '10003' '10016' '10008'\n",
      " '10016' '10010' '10013' '10006' '10008' '10000' '10010' '10003' '10001'\n",
      " '10006' '10000' '10001' '10013' '10005' '10010' '10005' '10002' '10008'\n",
      " '10018' '10005' '10002' '10008' '10008' '10003' '10015' '10008' '10005'\n",
      " '10008' '10014' '10006' '10003' '10006' '10003' '10006' '10003' '10005'\n",
      " '10008' '10003' '10013' '10000' '10015' '10005' '10006' '10005' '10003'\n",
      " '10000' '10001' '10000' '10012' '10018' '10000' '10017' '10003' '10003'\n",
      " '10000' '10008' '10005' '10003' '10004' '10015' '10004' '10005' '10019'\n",
      " '10006' '10006' '10008' '10001' '10005' '10000' '10014' '10000' '10003'\n",
      " '10013' '10015' '10002' '10013' '10010' '10003' '10000' '10000' '10005'\n",
      " '10016' '10003' '10008' '10008' '10002' '10003' '10016' '10003' '10000'\n",
      " '10001' '10001' '10000' '10005' '10000' '10015' '10000' '10003' '10003'\n",
      " '10008' '10010' '10003' '10000' '10003' '10002' '10015' '10002' '10010'\n",
      " '10005' '10003' '10005' '10019' '10003' '10002' '10002' '10005' '10003'\n",
      " '10000' '10015' '10005' '10002' '10000' '10001' '10003' '10000' '10019'\n",
      " '10000' '10019' '10000' '10008' '10002' '10003' '10005' '10013' '10019'\n",
      " '10005' '10003' '10005' '10000' '10018' '10015' '10000' '10000' '10013'\n",
      " '10008' '10013' '10002' '10010' '10015' '10015' '10005' '10003' '10003'\n",
      " '10006' '10003' '10003' '10010' '10003' '10003' '10004' '10008' '10003'\n",
      " '10006' '10006' '10010' '10003' '10008' '10003' '10018' '10000' '10000'\n",
      " '10000' '10005' '10001' '10005' '10003' '10013' '10005' '10003' '10003'\n",
      " '10003' '10000' '10006' '10008' '10002' '10015' '10005' '10000' '10005'\n",
      " '10005' '10005' '10008' '10010' '10003' '10019' '10000' '10003' '10005'\n",
      " '10015' '10013' '10006' '10000' '10008' '10006' '10003' '10003' '10006'\n",
      " '10017' '10001' '10011' '10002' '10010'] ['10006' '10011' '10003' '10005' '10010' '10002' '10013' '10005' '10013'\n",
      " '10003' '10003' '10000' '10005' '10001' '10000' '10001' '10003' '10002'\n",
      " '10001' '10012' '10008' '10001' '10003' '10005' '10013' '10013' '10005'\n",
      " '10005' '10006' '10000' '10008' '10014' '10000' '10003' '10007' '10004'\n",
      " '10006' '10019' '10011' '10008' '10003' '10015' '10016' '10013' '10003'\n",
      " '10010' '10002' '10003' '10001' '10008' '10001' '10003' '10003' '10002'\n",
      " '10015' '10009' '10013' '10005' '10002' '10017' '10006' '10014' '10014'\n",
      " '10009' '10004' '10003' '10005' '10007' '10015' '10001' '10003' '10003'\n",
      " '10001' '10008' '10008' '10014' '10000' '10003' '10011' '10005' '10003'\n",
      " '10013' '10006' '10008' '10016' '10003' '10000' '10008' '10006' '10010'\n",
      " '10005' '10000' '10005' '10013' '10005' '10002' '10001' '10015' '10016'\n",
      " '10000' '10018' '10003' '10000' '10013' '10009' '10003' '10001' '10003'\n",
      " '10000' '10003' '10000' '10000' '10006' '10000' '10016' '10002' '10015'\n",
      " '10014' '10003' '10000' '10003' '10008' '10008' '10005' '10003' '10000'\n",
      " '10003' '10017' '10000' '10000' '10014' '10003' '10003' '10000' '10000'\n",
      " '10009' '10014' '10002' '10003' '10005' '10005' '10016' '10003' '10008'\n",
      " '10008' '10008' '10006' '10005' '10018' '10015' '10000' '10001' '10006'\n",
      " '10005' '10005' '10001' '10006' '10015' '10013' '10001' '10010' '10015'\n",
      " '10000' '10015' '10018' '10002' '10000' '10003' '10005' '10003' '10008'\n",
      " '10010' '10000' '10013' '10008' '10002' '10010' '10003' '10010' '10000'\n",
      " '10007' '10019' '10005' '10001' '10000' '10014' '10006' '10005' '10013'\n",
      " '10000' '10003' '10015' '10003' '10000' '10015' '10003' '10005' '10008'\n",
      " '10000' '10015' '10003' '10001' '10014' '10006' '10010' '10006' '10015'\n",
      " '10000' '10002' '10016' '10001' '10000' '10006' '10013' '10015' '10008'\n",
      " '10001' '10000' '10013' '10000' '10017' '10002' '10000' '10002' '10002'\n",
      " '10005' '10006' '10003' '10008' '10002' '10000' '10001' '10000' '10001'\n",
      " '10000' '10003' '10006' '10003' '10003' '10009' '10002' '10000' '10006'\n",
      " '10000' '10000' '10000' '10012' '10001' '10006' '10006' '10000' '10018'\n",
      " '10001' '10000' '10003' '10000' '10006' '10017' '10006' '10013' '10013'\n",
      " '10003' '10003' '10000' '10015' '10000' '10013' '10003' '10000' '10000'\n",
      " '10008' '10005' '10019' '10001' '10004' '10015' '10006' '10019' '10013'\n",
      " '10008' '10019' '10018' '10003' '10002' '10003' '10000' '10018' '10008'\n",
      " '10000' '10005' '10000' '10016' '10010' '10000' '10010' '10003' '10001'\n",
      " '10006' '10000' '10006' '10016' '10013' '10001' '10003' '10005' '10016'\n",
      " '10000' '10005' '10013' '10003' '10000' '10000' '10000' '10002' '10006'\n",
      " '10003' '10000' '10012' '10003' '10001' '10001' '10001' '10008' '10005'\n",
      " '10016' '10013' '10003' '10002' '10015' '10002' '10000' '10005' '10001'\n",
      " '10000' '10002' '10000' '10015' '10001' '10016' '10016' '10005' '10001'\n",
      " '10015' '10008' '10011' '10003' '10013' '10013' '10003' '10006' '10001'\n",
      " '10003' '10006' '10006' '10006' '10006' '10000' '10018' '10013' '10013'\n",
      " '10002' '10015' '10000' '10002' '10003' '10002' '10003' '10005' '10005'\n",
      " '10015' '10003' '10003' '10008' '10002' '10000' '10008' '10003' '10003'\n",
      " '10003' '10011' '10000' '10003' '10003' '10002' '10002' '10000' '10013'\n",
      " '10008' '10000' '10002' '10014' '10000' '10005' '10015' '10001' '10003'\n",
      " '10003' '10005' '10005' '10000' '10003' '10002' '10000' '10002' '10018'\n",
      " '10000' '10005' '10003' '10003' '10000' '10005' '10008' '10000' '10000'\n",
      " '10015' '10006' '10000' '10003' '10016' '10006' '10008' '10006' '10003'\n",
      " '10004' '10006' '10005' '10000' '10016' '10015' '10000' '10000' '10000'\n",
      " '10003' '10012' '10013' '10001' '10015' '10003' '10000' '10003' '10005'\n",
      " '10016' '10001' '10000' '10005' '10019' '10000' '10000' '10003' '10003'\n",
      " '10003' '10006' '10000' '10000' '10008' '10005' '10006' '10013' '10000'\n",
      " '10000' '10000' '10001' '10005' '10016' '10013' '10003' '10000' '10006'\n",
      " '10002' '10006' '10019' '10001' '10018' '10000' '10005' '10003' '10000'\n",
      " '10003' '10019' '10008' '10005' '10008' '10018' '10000' '10003' '10000'\n",
      " '10015' '10017' '10009' '10005' '10008' '10004' '10003' '10003' '10008'\n",
      " '10003' '10003' '10003' '10002' '10010']\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 1. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "0.27\n",
      "0.27\n",
      "0.27\n",
      "0.27\n",
      "0.16594113641366354\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.35      0.52      0.42        62\n",
      "       10001       0.28      0.31      0.29        32\n",
      "       10002       0.22      0.23      0.23        30\n",
      "       10003       0.26      0.22      0.23       102\n",
      "       10004       0.00      0.00      0.00         5\n",
      "       10005       0.31      0.29      0.30        49\n",
      "       10006       0.21      0.18      0.20        44\n",
      "       10007       0.00      0.00      0.00         1\n",
      "       10008       0.39      0.32      0.35        41\n",
      "       10009       0.00      0.00      0.00         3\n",
      "       10010       0.27      0.20      0.23        15\n",
      "       10011       0.00      0.00      0.00         2\n",
      "       10012       0.00      0.00      0.00         6\n",
      "       10013       0.14      0.17      0.15        23\n",
      "       10014       0.10      0.20      0.13         5\n",
      "       10015       0.73      0.53      0.61        36\n",
      "       10016       0.06      0.09      0.07        11\n",
      "       10017       0.00      0.00      0.00        11\n",
      "       10018       0.10      0.10      0.10        10\n",
      "       10019       0.00      0.00      0.00        12\n",
      "\n",
      "   micro avg       0.27      0.27      0.27       500\n",
      "   macro avg       0.17      0.17      0.17       500\n",
      "weighted avg       0.28      0.27      0.27       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(y_test, predicted_y)\n",
    "print(model.predict_proba(X_test))\n",
    "print(accuracy_score(y_test, predicted_y))\n",
    "print(precision_score(y_test, predicted_y, average='micro'))\n",
    "print(recall_score(y_test, predicted_y, average='micro'))\n",
    "print(f1_score(y_test, predicted_y, average='micro', labels = np.unique(predicted_y)))\n",
    "print(f1_score(y_test, predicted_y, average='macro', labels = np.unique(predicted_y)))\n",
    "print(classification_report(y_test, predicted_y,output_dict= False, labels = np.unique(predicted_y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
