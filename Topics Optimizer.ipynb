{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import csv\n",
    "from pprint import pprint\n",
    "import collections\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.tsv', sep='\\t', quoting=csv.QUOTE_NONE, dtype=str, encoding = 'utf-8',\n",
    "                 header=None, names=[\"instance\", \"text\", \"id\", \"sentiment\", \"is_sarcastic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \" \", sample)\n",
    "\n",
    "\n",
    "def remove_punctuation(sample):\n",
    "    \"\"\"Remove punctuations from a sample string\"\"\"\n",
    "    return re.sub(r'[^\\w\\s\\&\\#\\@\\$\\%\\_]', '', sample)\n",
    "\n",
    "\n",
    "def myTokenizer(sample):\n",
    "    \"\"\"Customized tokenizer\"\"\"\n",
    "    new_words = []\n",
    "    words = sample.split(' ')\n",
    "    new_words = [word for word in words if len(word) >= 2 and not word.isdigit() and not word.startswith('#aus') and not word.startswith('au')]\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_stopwords_NLTK(sample):\n",
    "    \"\"\"Remove stopwords using NLTK\"\"\"\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    filteredText = \"\"\n",
    "    for word in words:\n",
    "        if word not in stopWords:\n",
    "            filteredText = filteredText + word + \" \"\n",
    "    return filteredText.rstrip()\n",
    "\n",
    "\n",
    "def porter_stem(sample):\n",
    "    \"\"\"Stemming\"\"\"\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + ps.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "\n",
    "def lemmy(sample):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    lemmed_text = \"\"\n",
    "    for word in words:\n",
    "        lemmed_text = lemmed_text + lemmatizer.lemmatize(word, pos='v') + \" \"\n",
    "    return lemmed_text.rstrip()\n",
    "\n",
    "\n",
    "def snowball(sample):\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + stemmer.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "\n",
    "def myPreprocessor(sample):\n",
    "    \"\"\"Customized preprocessor\"\"\"\n",
    "    sample = remove_URL(sample)\n",
    "    sample = sample.lower()\n",
    "    sample = remove_stopwords_NLTK(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "    #sample = lemmy(sample)\n",
    "    sample = porter_stem(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(input_text):\n",
    "    return re.sub(r'@\\w+', '', input_text)\n",
    "\n",
    "def remove_urls(input_text):\n",
    "    return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "\n",
    "def emoji_oneword(input_text):\n",
    "    # By compressing the underscore, the emoji is kept as one word\n",
    "    return input_text.replace('_','')\n",
    "\n",
    "def remove_punctuation(input_text):\n",
    "    # Make translation table\n",
    "    punct = string.punctuation\n",
    "    trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "    return input_text.translate(trantab)\n",
    "\n",
    "def remove_digits(input_text):\n",
    "    return re.sub('\\d+', '', input_text)\n",
    "\n",
    "def to_lower(input_text):\n",
    "    return input_text.lower()\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) \n",
    "\n",
    "def stemming(input_text):\n",
    "    porter = PorterStemmer()\n",
    "    words = input_text.split() \n",
    "    stemmed_words = [porter.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "def newProcess(sample):\n",
    "    sample = remove_mentions(sample)\n",
    "    sample = remove_urls(sample)\n",
    "    sample = emoji_oneword(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "    sample = remove_digits(sample)\n",
    "    sample = to_lower(sample)\n",
    "    sample = remove_stopwords(sample)\n",
    "    sample = stemming(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data creation \"\"\"\n",
    "text_data = np.array([])\n",
    "# Read tweets\n",
    "for text in df.text:\n",
    "    text_data = np.append(text_data, text)\n",
    "# creating target classes\n",
    "Y = np.array([])\n",
    "for text in df.id:\n",
    "    Y = np.append(Y, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_data, Y, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "def grid_vect(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None):\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect', vect)\n",
    "        , ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)\n",
    "\n",
    "    # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=2, scoring='f1_micro')\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best CV score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_search.best_estimator_.score(X_test, y_test))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report Test Data\")\n",
    "    print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "                        \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_vect = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'vect__min_df': (1,2,3,4,5),\n",
    "    'vect__max_features': (600,700,800,1000,1200,1400),\n",
    "    'vect__max_df': (0.2, 0.4, 0.6, 0.8 ,1.0),\n",
    "    'vect__preprocessor': (None, myPreprocessor),\n",
    "    'vect__tokenizer': (None, myTokenizer)\n",
    "}\n",
    "\n",
    "\n",
    "# Parameter grid settings for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.25, 0.5, 0.6 ,0.75, 1.0)\n",
    "}\n",
    "\n",
    "parameters_rf = {\n",
    "    #'clf__bootstrap': [True, False],\n",
    "    'clf__max_depth': [60, 70, 80, 90, 100, 200,None],\n",
    "    'clf__max_features': ['auto', 'sqrt'],\n",
    "    'clf__min_samples_leaf': [1, 2, 4, 8],\n",
    "    'clf__min_samples_split': [2, 5, 10],\n",
    "    'clf__n_estimators': [200,400,600, 800, 1000]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocessor=myPreprocessor, tokenizer= myTokenizer\n",
    "#best_mnb_countvect = grid_vect(MultinomialNB(), parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=CountVectorizer(preprocessor=newProcess, tokenizer= myTokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['bootstrap', 'class_weight', 'criterion', 'max_depth', 'max_features', 'max_leaf_nodes', 'min_impurity_decrease', 'min_impurity_split', 'min_samples_leaf', 'min_samples_split', 'min_weight_fraction_leaf', 'n_estimators', 'n_jobs', 'oob_score', 'random_state', 'verbose', 'warm_start'])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RandomForestClassifier().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__max_depth': [60, 70, 80, 90, 100, 200, None],\n",
      " 'clf__max_features': ['auto', 'sqrt'],\n",
      " 'clf__min_samples_leaf': [1, 2, 4, 8],\n",
      " 'clf__min_samples_split': [2, 5, 10],\n",
      " 'clf__n_estimators': [200, 400, 600, 800, 1000]}\n",
      "Fitting 2 folds for each of 840 candidates, totalling 1680 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   12.3s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   59.1s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  2.3min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  6.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1680 out of 1680 | elapsed:  9.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 554.373s\n",
      "\n",
      "Best CV score: 0.549\n",
      "Best parameters set:\n",
      "\tclf__max_depth: 80\n",
      "\tclf__max_features: 'auto'\n",
      "\tclf__min_samples_leaf: 2\n",
      "\tclf__min_samples_split: 5\n",
      "\tclf__n_estimators: 200\n",
      "Test score with best_estimator_: 0.570\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.72      0.70      0.71        56\n",
      "       10001       0.48      0.42      0.45        36\n",
      "       10002       0.53      0.58      0.55        31\n",
      "       10003       0.36      0.61      0.45        87\n",
      "       10004       0.00      0.00      0.00         2\n",
      "       10005       0.79      0.73      0.76        52\n",
      "       10006       0.54      0.43      0.48        44\n",
      "       10007       0.00      0.00      0.00         2\n",
      "       10008       0.73      0.80      0.76        46\n",
      "       10009       0.67      1.00      0.80         4\n",
      "       10010       0.70      0.64      0.67        11\n",
      "       10011       0.50      0.14      0.22         7\n",
      "       10012       0.50      0.50      0.50         4\n",
      "       10013       0.88      0.62      0.73        37\n",
      "       10014       0.00      0.00      0.00         6\n",
      "       10015       0.67      0.67      0.67        24\n",
      "       10016       0.75      0.21      0.33        14\n",
      "       10017       0.29      0.17      0.21        12\n",
      "       10018       0.75      0.30      0.43        10\n",
      "       10019       0.71      0.33      0.45        15\n",
      "\n",
      "   micro avg       0.57      0.57      0.57       500\n",
      "   macro avg       0.53      0.44      0.46       500\n",
      "weighted avg       0.60      0.57      0.57       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_random = grid_vect(RandomForestClassifier(), parameters_rf, X_train, X_test, parameters_text=None, vect=CountVectorizer(preprocessor=newProcess, tokenizer= myTokenizer, max_features=1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
