{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "from time import time\n",
    "import re\n",
    "import string\n",
    "import os\n",
    "import csv\n",
    "from pprint import pprint\n",
    "import collections\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('dataset.tsv', sep='\\t', quoting=csv.QUOTE_NONE, dtype=str, encoding = 'utf-8',\n",
    "                 header=None, names=[\"instance\", \"text\", \"id\", \"sentiment\", \"is_sarcastic\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_URL(sample):\n",
    "    \"\"\"Remove URLs from a sample string\"\"\"\n",
    "    return re.sub(r\"http\\S+\", \" \", sample)\n",
    "\n",
    "\n",
    "def remove_punctuation(sample):\n",
    "    \"\"\"Remove punctuations from a sample string\"\"\"\n",
    "    return re.sub(r'[^\\w\\s\\&\\#\\@\\$\\%\\_]', '', sample)\n",
    "\n",
    "\n",
    "def myTokenizer(sample):\n",
    "    \"\"\"Customized tokenizer\"\"\"\n",
    "    new_words = []\n",
    "    words = sample.split(' ')\n",
    "    new_words = [word for word in words if len(word) >= 2 and not word.isdigit() and not word.startswith('#aus') and not word.startswith('au')]\n",
    "    return new_words\n",
    "\n",
    "\n",
    "def remove_stopwords_NLTK(sample):\n",
    "    \"\"\"Remove stopwords using NLTK\"\"\"\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    filteredText = \"\"\n",
    "    for word in words:\n",
    "        if word not in stopWords:\n",
    "            filteredText = filteredText + word + \" \"\n",
    "    return filteredText.rstrip()\n",
    "\n",
    "\n",
    "def porter_stem(sample):\n",
    "    \"\"\"Stemming\"\"\"\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    ps = PorterStemmer()\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + ps.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "\n",
    "def lemmy(sample):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    lemmed_text = \"\"\n",
    "    for word in words:\n",
    "        lemmed_text = lemmed_text + lemmatizer.lemmatize(word, pos='v') + \" \"\n",
    "    return lemmed_text.rstrip()\n",
    "\n",
    "\n",
    "def snowball(sample):\n",
    "    words = [w for w in sample.split(' ') if len(w) >= 2]\n",
    "    stemmer = SnowballStemmer(\"english\")\n",
    "    stemmed_text = \"\"\n",
    "    for word in words:\n",
    "        stemmed_text = stemmed_text + stemmer.stem(word) + \" \"\n",
    "    return stemmed_text.rstrip()\n",
    "\n",
    "\n",
    "def myPreprocessor(sample):\n",
    "    \"\"\"Customized preprocessor\"\"\"\n",
    "    sample = remove_URL(sample)\n",
    "    sample = sample.lower()\n",
    "    sample = remove_stopwords_NLTK(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "    #sample = lemmy(sample)\n",
    "    sample = porter_stem(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_mentions(input_text):\n",
    "    return re.sub(r'@\\w+', '', input_text)\n",
    "\n",
    "def remove_urls(input_text):\n",
    "    return re.sub(r'http.?://[^\\s]+[\\s]?', '', input_text)\n",
    "\n",
    "def emoji_oneword(input_text):\n",
    "    # By compressing the underscore, the emoji is kept as one word\n",
    "    return input_text.replace('_','')\n",
    "\n",
    "def remove_punctuation(input_text):\n",
    "    # Make translation table\n",
    "    punct = string.punctuation\n",
    "    trantab = str.maketrans(punct, len(punct)*' ')  # Every punctuation symbol will be replaced by a space\n",
    "    return input_text.translate(trantab)\n",
    "\n",
    "def remove_digits(input_text):\n",
    "    return re.sub('\\d+', '', input_text)\n",
    "\n",
    "def to_lower(input_text):\n",
    "    return input_text.lower()\n",
    "\n",
    "def remove_stopwords(input_text):\n",
    "    stopwords_list = stopwords.words('english')\n",
    "    # Some words which might indicate a certain sentiment are kept via a whitelist\n",
    "    whitelist = [\"n't\", \"not\", \"no\"]\n",
    "    words = input_text.split() \n",
    "    clean_words = [word for word in words if (word not in stopwords_list or word in whitelist) and len(word) > 1] \n",
    "    return \" \".join(clean_words) \n",
    "\n",
    "def stemming(input_text):\n",
    "    porter = PorterStemmer()\n",
    "    words = input_text.split() \n",
    "    stemmed_words = [porter.stem(word) for word in words]\n",
    "    return \" \".join(stemmed_words)\n",
    "\n",
    "def newProcess(sample):\n",
    "    sample = remove_mentions(sample)\n",
    "    sample = remove_urls(sample)\n",
    "    sample = emoji_oneword(sample)\n",
    "    sample = remove_punctuation(sample)\n",
    "    sample = remove_digits(sample)\n",
    "    sample = to_lower(sample)\n",
    "    sample = remove_stopwords(sample)\n",
    "    sample = stemming(sample)\n",
    "    return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Data creation \"\"\"\n",
    "text_data = np.array([])\n",
    "# Read tweets\n",
    "for text in df.text:\n",
    "    text_data = np.append(text_data, text)\n",
    "# creating target classes\n",
    "Y = np.array([])\n",
    "for text in df.id:\n",
    "    Y = np.append(Y, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(text_data, Y, test_size=0.25, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based on http://scikit-learn.org/stable/auto_examples/model_selection/grid_search_text_feature_extraction.html\n",
    "def grid_vect(clf, parameters_clf, X_train, X_test, parameters_text=None, vect=None):\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('vect', vect)\n",
    "        , ('clf', clf)\n",
    "    ])\n",
    "    \n",
    "    # Join the parameters dictionaries together\n",
    "    parameters = dict()\n",
    "    if parameters_text:\n",
    "        parameters.update(parameters_text)\n",
    "    parameters.update(parameters_clf)\n",
    "\n",
    "    # Make sure you have scikit-learn version 0.19 or higher to use multiple scoring metrics\n",
    "    grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1, verbose=1, cv=5, scoring='f1_micro')\n",
    "    \n",
    "    print(\"Performing grid search...\")\n",
    "    print(\"pipeline:\", [name for name, _ in pipeline.steps])\n",
    "    print(\"parameters:\")\n",
    "    pprint(parameters)\n",
    "\n",
    "    t0 = time()\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    print(\"done in %0.3fs\" % (time() - t0))\n",
    "    print()\n",
    "\n",
    "    print(\"Best CV score: %0.3f\" % grid_search.best_score_)\n",
    "    print(\"Best parameters set:\")\n",
    "    best_parameters = grid_search.best_estimator_.get_params()\n",
    "    for param_name in sorted(parameters.keys()):\n",
    "        print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))\n",
    "        \n",
    "    print(\"Test score with best_estimator_: %0.3f\" % grid_search.best_estimator_.score(X_test, y_test))\n",
    "    print(\"\\n\")\n",
    "    print(\"Classification Report Test Data\")\n",
    "    print(classification_report(y_test, grid_search.best_estimator_.predict(X_test)))\n",
    "                        \n",
    "    return grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter grid settings for the vectorizers (Count and TFIDF)\n",
    "parameters_vect = {\n",
    "    'vect__ngram_range': ((1, 1), (1, 2)),\n",
    "    'vect__min_df': (1,2,3,4,5),\n",
    "    'vect__max_features': (600,700,800,1000,1200,1400),\n",
    "    'vect__max_df': (0.2, 0.4, 0.6, 0.8 ,1.0)\n",
    "#     'vect__preprocessor':(None, myPreprocessor),\n",
    "#     'vect__tokenizer':(None, myTokenizer)\n",
    "}\n",
    "\n",
    "\n",
    "# Parameter grid settings for MultinomialNB\n",
    "parameters_mnb = {\n",
    "    'clf__alpha': (0.25, 0.5, 0.6 ,0.75, 1.0)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['vect', 'clf']\n",
      "parameters:\n",
      "{'clf__alpha': (0.25, 0.5, 0.6, 0.75, 1.0),\n",
      " 'vect__max_df': (0.2, 0.4, 0.6, 0.8, 1.0),\n",
      " 'vect__max_features': (600, 700, 800, 1000, 1200, 1400),\n",
      " 'vect__min_df': (1, 2, 3, 4, 5),\n",
      " 'vect__ngram_range': ((1, 1), (1, 2))}\n",
      "Fitting 5 folds for each of 1500 candidates, totalling 7500 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  26 tasks      | elapsed:   11.7s\n",
      "[Parallel(n_jobs=-1)]: Done 176 tasks      | elapsed:   51.7s\n",
      "[Parallel(n_jobs=-1)]: Done 426 tasks      | elapsed:  2.0min\n",
      "[Parallel(n_jobs=-1)]: Done 776 tasks      | elapsed:  3.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1226 tasks      | elapsed:  5.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1776 tasks      | elapsed:  8.3min\n",
      "[Parallel(n_jobs=-1)]: Done 2426 tasks      | elapsed: 11.3min\n",
      "[Parallel(n_jobs=-1)]: Done 3176 tasks      | elapsed: 14.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4026 tasks      | elapsed: 18.6min\n",
      "[Parallel(n_jobs=-1)]: Done 4976 tasks      | elapsed: 23.0min\n",
      "[Parallel(n_jobs=-1)]: Done 6026 tasks      | elapsed: 28.0min\n",
      "[Parallel(n_jobs=-1)]: Done 7176 tasks      | elapsed: 33.3min\n",
      "[Parallel(n_jobs=-1)]: Done 7500 out of 7500 | elapsed: 34.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2085.643s\n",
      "\n",
      "Best CV score: 0.470\n",
      "Best parameters set:\n",
      "\tclf__alpha: 0.75\n",
      "\tvect__max_df: 0.2\n",
      "\tvect__max_features: 700\n",
      "\tvect__min_df: 4\n",
      "\tvect__ngram_range: (1, 1)\n",
      "Test score with best_estimator_: 0.484\n",
      "\n",
      "\n",
      "Classification Report Test Data\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       10000       0.61      0.66      0.63        56\n",
      "       10001       0.38      0.28      0.32        36\n",
      "       10002       0.54      0.42      0.47        31\n",
      "       10003       0.35      0.55      0.42        87\n",
      "       10004       0.00      0.00      0.00         2\n",
      "       10005       0.61      0.63      0.62        52\n",
      "       10006       0.41      0.41      0.41        44\n",
      "       10007       0.00      0.00      0.00         2\n",
      "       10008       0.60      0.74      0.66        46\n",
      "       10009       0.00      0.00      0.00         4\n",
      "       10010       0.27      0.27      0.27        11\n",
      "       10011       0.00      0.00      0.00         7\n",
      "       10012       0.00      0.00      0.00         4\n",
      "       10013       0.77      0.62      0.69        37\n",
      "       10014       0.00      0.00      0.00         6\n",
      "       10015       0.57      0.67      0.62        24\n",
      "       10016       0.20      0.07      0.11        14\n",
      "       10017       0.00      0.00      0.00        12\n",
      "       10018       0.50      0.30      0.37        10\n",
      "       10019       0.38      0.20      0.26        15\n",
      "\n",
      "   micro avg       0.48      0.48      0.48       500\n",
      "   macro avg       0.31      0.29      0.29       500\n",
      "weighted avg       0.46      0.48      0.46       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# preprocessor=myPreprocessor, tokenizer= myTokenizer\n",
    "best_mnb_countvect = grid_vect(MultinomialNB(), parameters_mnb, X_train, X_test, parameters_text=parameters_vect, vect=CountVectorizer(preprocessor=newProcess, tokenizer= myTokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
